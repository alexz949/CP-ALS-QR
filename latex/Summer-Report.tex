\documentclass{article}
\title{CP-ALS-QR report}
\author{Alex Zhang}
\date{July 2023}
\textwidth=16.00cm 
\textheight=26.00cm 
\topmargin=0.00cm
\oddsidemargin=0.00cm 
\evensidemargin=0.00cm 
\headheight=0cm 
\headsep=0.5cm
\textheight=610pt
\usepackage{graphicx}
\usepackage{lineno,hyperref}
\usepackage{amsmath,amssymb,enumerate,graphicx,pifont,color,tikz,yfonts,scalerel,changepage,algorithm,algpseudocode}
\usepackage{bookmark}
\usepackage{diagbox}
\usepackage{caption,subcaption}


\usetikzlibrary{positioning}
\usetikzlibrary{cd}

\usepackage{booktabs}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{array}
\usepackage{tikz}
\usepackage{cleveref}


\graphicspath{{fig/}}

\usepackage{latexsym,array,delarray,amsthm,amssymb,epsfig}
\usepackage{amsmath}
\usepackage{listings}
\lstset{
  basicstyle=\ttfamily,
  mathescape
}

\newcommand{\bmat}[1]{\begin{bmatrix} #1 \end{bmatrix}}
\newcommand{\mat}[1]{\mathbf{#1}}
\newcommand{\ten}[1]{\mathcal{#1}}
% matrix/vector/tensor/element macros
\usepackage{bm}
\newcommand{\Tra}{T}										% transpose
\newcommand{\M}[2][]{\bm{#1{\mathbf{\MakeUppercase{#2}}}}} 		% matrix
\newcommand{\Me}[3][]{\bm{#1{\mathbf{\MakeUppercase{#2}}}}({#3})} 		% matrix entry
\newcommand{\Mb}[3][]{\bm{#1{\mathbf{\MakeUppercase{#2}}}}_{#3}}       	% submatrix
\newcommand{\Mbe}[4][]{\bm{#1{\mathbf{\MakeUppercase{#2}}}}_{#3}({#4})}	% submatrix entry
\newcommand{\Ms}[3][]{\bm{#1{\mathbf{\MakeUppercase{#2}}}}^{(#3)}}       	% matrix in series
\newcommand{\Mbs}[4][]{\bm{#1{\mathbf{\MakeUppercase{#2}}}}_{#3}^{(#4)}}   % submatrix in series
\newcommand{\V}[2][]{\bm{#1{\mathbf{\MakeLowercase{#2}}}}} 		% vector
\newcommand{\Vs}[3][]{\bm{#1{\mathbf{\MakeLowercase{#2}}}}^{(#3)}} 		% vector in series
\newcommand{\Ve}[3][]{\bm{#1{\mathbf{\MakeLowercase{#2}}}}({#3})}		% vector entry
\newcommand{\T}[2][]{#1{\mathbf{\cal{#2}}}} 						% tensor
\newcommand{\Te}[3][]{#1{\mathbf{\cal{#2}}}({#3})}		

\let\ds\displaystyle

\newcommand{\GB}[1]{\textcolor{red}{\textbf{GB}: #1}}


\begin{document}



\maketitle
\section{Introduction}
%The CANDECOMP/PARAFAC or canonical polyadic (CP) decomposition for multidimensional data, 
%or tensors, is a popular tool for analyzing and interpreting latent patterns that may be 
%present in multidimensional data. Basically CP decomposition of a tensor refers to its
%expression as a sum of $r$ rank-one components and each of them is a vector outer product.
%One of the most popular methods used to compute a CP decomposition is the alternating least
%squares (CP-ALS) approach, which solves a series of linear least squares problems. Usually
%to solve these linear leaste squares problems, normal equations are used for CP-ALS. This
%approach may be sensitive for ill-conditioned inputs. Based on this idea, there are already
%a more stable approach which is solving the linear least sqaures problems using QR decomposition
%instead.

%For my summer research project, I basically follows the QR apprach but trying to 
%improve the efficiency for QR decomposition when assuming the input tensor is in Kruskal structure,
%that is, a tensor stored as factor matrices and corresponding weights. By exploiting this structure, 
%we improve the computation efficiency by not forming Multi-TTM tensor.
%The problem left is when doing CP-ALS, QR-based methods is exponential in $N$, the number of modes.
%The normal equations approach is linear in $N$ for Kruskal tensor. During the summer I tried
%to revise and implement former QR method which archieved better stability than normal equations
%but computation time increases linearly with respect to $N$.






\section{Background}
\subsection{CP Decomposition}
Given a $d$-way tensor $\T{X} \in \mathbb{R}^{n_1\times n_2\times \dots \times n_d}$, its
CP decomposition $\T{K}$ of rank $r \in \mathbb{N}$ can be represented as 
\begin{equation}
\label{eq:CP}
\T{X}(i_1,i_2,\dots, i_d) \approx \T{K} = \sum^{r}_{j=1} \V{\lambda}(j) \mat{A_1}(i_1,j)\mat{A_2}(i_2,j) \dots \mat{A_d}(i_d,j)
\end{equation}
for all $(i_1,i_2,\dots, i_d) \in [n_1] \otimes [n_2] \otimes [n_3] \otimes \dots \otimes [n_d]$ where $\mat{A_k} \in \mathbb{R}^{n_k \times r}$ is a factor matrix for each $k \in [d]$ and $\bm{\lambda}\in\mathbb{R}^r$ is a vector of weights. 
\Cref{fig:3d-cp-decomp} gives a visualization of a CP Decomposition for a 3-way tensor.

\begin{figure}[ht!]
\centering
\input{tikz/cp.tex}
\caption{CP decomposition of rank $R$ for a three-dimensional tensor $\T{X}$ \label{fig:3d-cp-decomp}}
\end{figure}
We can represent $\T[]{K}$ in shorthand CP notation as $\T{K} = [\![\V{\lambda};\mat{A}_1,\mat{A}_2, \dots,\mat{A}_d ]\!]$. 
There exists ambiguity in the weighting of the columns of the factor matrices and $\V{\lambda}$ vector.
For simplicity of presentation, we drop the weight vector $\V{\lambda}$ throughout this report and use the representation
$$\T{K} = [\![\mat{A}_1,\mat{A}_2, \dots,\mat{A}_d ]\!].$$

\subsection{Linear Least Squares Problem}
\label{sec:LS}

In mathematical form, a generic linear least squares problem is given by 
\begin{equation}
\label{eq:LS}
\min_{\mat{X}}||\mat{B} - \mat{X}\mat{A}^\top||_{F}.
\end{equation}
Note that the coefficient matrix $\mat{A}$ appears on the right of the variable matrix, which is the opposite of the standard form.
We use this form to relate to the corresponding least squares problem that arises in the context of computing a CP decomposition.

Several different techniques can be used to solve this problem.
The two we consider here are via the Normal Equations (NE) and by QR decomposition.
The NE specify a (square) linear system of equations and are derived from setting the gradient of the objective function from \cref{eq:LS} to zero, and they involve the Gram of $\mat{A}$ as well as the product $\mat{B}\mat{A}$:
\begin{equation}
\label{eq:NE}
\mat{X}\mat{A}^\top\mat{A} = \mat{B}\mat{A}.
\end{equation}
The NE can be solved via Cholesky factorization of $\mat{A}^\top\mat{A}$ and triangular solves.
The cost of NE depends on the size of $\mat{A}$ and $\mat{B}$. 
For $\mat{A} \in \mathbb{R}^{m \times n}$ and $\mat{B} \in \mathbb{R}^{p \times m}$, the arithmetic complexity of each step of the NE approach is given in \cref{tab:NE-time}.
\begin{table}[!ht]
  \centering
  \begin{tabular}{|c|c|}
    \hline
    Components & Number of flops\\
    \hline
    Compute Gram & $mn^2$ \\
    Compute $\mat{B}\mat{A}$ & $2mnp$\\
    Cholesky factorization & $1/3n^3$ \\
    Back solves & $2pn^2$ \\
    \hline
  \end{tabular}
  \caption{Breakdown of time for using NE to solve \cref{eq:LS} for $\mat{A} \in \mathbb{R}^{m \times n}$ and $\mat{B} \in \mathbb{R}^{p \times m}$}
  \label{tab:NE-time}
\end{table}

In the QR decomposition approach, we first compute the compact QR decomposition $\mat{A} = \mat{Q}\mat{R}$ (so that $\mat{Q}\in \mathbb{R}^{m \times n}$)and then solve the  triangular system
\begin{align}
\label{eq:QR}  
\mat{X} \mat{R}^\top = \mat{B}\mat{Q}.
\end{align}
The time complexity of each step of this approach is given in \cref{tab:QR-time}.
Note that the cost of the QR decomposition assumes that Householder QR is used in the factorization and that the $\mat{Q}$ matrix is formed explicitly.
If $p$ is smaller than $n$, then it is more efficient to leave $\mat{Q}$ in implicit Householder form, reducing the cost of QR to $2mn^2-2/3n^3$.
In this case, we apply $\mat{Q}$ to $\mat{B}$ using Householder transformations, which increases the cost of computing $\mat{B}\mat{Q}$ to $4mnp$. 
%\begin{align}
%  &arg\min_{\mat{X}}||\mat{B} - \mat{X}\mat{A}^\top||_{F}^2 \\
%  &= arg \min_{\mat{X}}||\mat{B} - \mat{X}(\mat{Q}\mat{R})^\top||_{F}^2 \\
%  &= arg \min_{\mat{X}}||\big[\mat{B} - \mat{X}\mat{R}^\top\mat{Q}^\top\big]\big[\mat{Q} \mat{Q}^\bot\big]||_{F}^2 \\
%  &= arg \min_{\mat{X}}||\big[\mat{B}\mat{Q} - \mat{X}\mat{R}^\top \text{    } \mat{B} \mat{Q}^\bot\big]||_{F}^2 \\
%  &= arg \min_{\mat{X}}||\mat{B}\mat{Q} - \mat{X}\mat{R}^\top||_{F}^2 + ||\mat{B}\mat{Q}^\bot||_{F}^2
%\end{align}

\begin{table}[!ht]
  \centering
  \begin{tabular}{|c|c|}
    \hline
    Components & Number of flops\\
    \hline
    Compute QR & $4mn^2 - 4/3n^3$ \\
    Compute $\mat{B}\mat{Q}$ & $2mnp$\\
    N/A & \\
    Back solves & $pn^2$ \\
    \hline
  \end{tabular}
  \caption{Breakdown of time for using QR to solve \cref{eq:LS} for $\mat{A} \in \mathbb{R}^{m \times n}$ and $\mat{B} \in \mathbb{R}^{p \times m}$}
  \label{tab:QR-time}
\end{table}

If $\mat{B}$ has few columns ($p\ll n$), NE is cheaper because of the smaller coefficient in the leading order cost ($mn^2$ vs $2mn^2$). 
However, when $\mat{B}$ has many columns ($p\gg n$), the largest cost is computing $\mat{B}\mat{A}$ for NE and $\mat{B}\mat{Q}$ for QR factorization, which have the same leading order cost and same constant ($2mnp$).

Using QR is more numerically stable than NE. 
Let $\kappa = \sigma_1(\mat{A})/\sigma_n(\mat{A})$ be the standard condition number of $\mat{A}$.
If we use QR to solve a least squares problem with a single right hand side; i.e. $\min\|\mat{A}\V{x} - \V{b} \|$, then from \cite[Eq. (19.2)]{trefethen1997numerical} the relative forward error satisfies
\begin{equation}
  \frac{||\tilde{x} - x||}{||x||} = O \biggl(\biggl(\kappa + \frac{\kappa^2\tan\theta}{\eta}\biggr)\epsilon_{mach}\biggr),
\end{equation}
where $\V[\tilde]{x}$ is the computed solution, $\V{x}$ is the exact solution, $\theta$ is the angle between $\mat{A}\V{x}$ and $\V{b}$, and $\eta = \|\mat{A}\|\|\V{x}\|/\|\mat{A}\V{x}\|$.
However, if we use the NE approach, then by \cite[Eq. (19.3)]{trefethen1997numerical} the forward error guarantee can be much larger: 
\begin{equation}
  \frac{||\tilde{x} - x||}{||x||} = O \biggl(\kappa^2 \epsilon_{\text{mach}}\biggr).
\end{equation}
Because of the numerical sensitivity introduced by using the NE approach, the QR approach is much more accurate when $\kappa$ is large (e.g., on the order of $\epsilon_{\text{mach}}^{-1/2}$ or larger).


\subsection{CP-ALS}

\subsubsection{CP Optimization Problem}

When computing a rank-$r$ CP approximation $\T{K}$ for a N-way tensor $\T{X}$, we seek to minimize $\| \T{X} - \T{K} \|$, where the tensor norm generalizes the vector 2-norm or the matrix Frobenius norm.
Expressed using \cref{eq:CP}, we have  
$$\min_{\mat{A}_1,\mat{A}_2, \dots,\mat{A}_d} \sum_{i_1=1}^{n_1} \cdots \sum_{i_d=1}^{n_d} \left(\T{X}(i_1,i_2,\dots, i_d) - \sum^{r}_{j=1} \mat{A_1}(i_1,j)\mat{A_2}(i_2,j) \dots \mat{A_d}(i_d,j) \right)^2 $$
However, this is a nonlinear, nonconvex optimization problem and no closed form solution exists. 
We must resort to using iterative methods to approximate a solution \cite{kolda2009tensor}.

\subsubsection{CP-ALS}

One of the most effective algorithms for computing a CP decomposition of a tensor is called Alternating Least Squares (CP-ALS).
The algorithm works as a block coordinate descent method, where each block of variables is a factor matrix.
That is, it alternates over the factor matrices, holding all but one fixed and computing the optimal solution for the variable matrix.

\GB{Give CP-ALS algorithm here with least squares problem formulation (not using NE)}


The idea is to unfold $\T[]{X}$ and $\T[]{K}$ along all modes so that we can solve a linear least squares problem for each factor matrix.
$$\min||\T[]{X} - \T[]{K}|| = \min||\mat{X}_{(n)} - \mat{K}_{(n)}||$$
where $\mat{K}_{(n)} = \hat{\mat{A}}_n(\mat{A}_N \odot \dots \odot \mat{A}_{n+1} \odot \mat{A}_{n-1} \odot \dots \odot \mat{A}_1)^\top$, $\hat{\mat{A}}_n = \mat{A}_n \cdot diag(\lambda)$.
Let $\mat{Z}_n=\mat{A}_N \odot \dots \odot \mat{A}_{n+1} \odot \mat{A}_{n-1} \odot \dots \odot \mat{A}_1$, then $\mat{K}_{(n)} = \hat{\mat{A}}_n\mat{Z}_n^\top$.
We can rewrite this minimize problem to be a least square problem for $\hat{\mat{A}}_n$.
\begin{equation}
  \min_{\hat{\mat{A}}_n}\|\mat{X}_{(n)}-\hat{\mat{A}}_n\mat{Z}_n^\top\|
\end{equation}

\subsubsection{CP-Rounding}

\GB{I started to edit this section, but it's in rough shape.  You don't want to introduce the CP notation here, that should be done earlier in the CP Decomposition section.  You should introduce what notation you'll use to distinguish between input and output.  You should use B's to represent the input to match what you do later in the QR section.  But you need to show how NE works with Kruskal input.  You can leave some details out, but you need the expressions so we can do the complexity analysis for comparison.  You can cite the Kolda-Bader paper on sparse and structured tensors here.  Also, don't switch to 3-way tensors here, you're doing $d$-way everywhere else so stick to that.}

For some applications, we wish to compute a CP decomposition of a matrix that is already in CP or \emph{Kruskal} format.
That is, we wish to compute a lower-rank representation of the input tensor, sometimes referred to a CP-Rounding.
Given a d-way tensor $\T[]{X} \in \mathbb{R}^{I_1 \times \dots \times I_d}$, it can be written as a sum of $R$ rank-1 tensors.
$$\T[]{X} = \sum^R_{i=1}\lambda_i\V{b}_i^{(1)} \circ \dots \circ \V{b}_i^{(d)}$$
This is referred as \emph{Kruskal} tensor. It's shorthand notation will be 

$$\T{X} = [\![\bm{\lambda} ; \mat{B}^{(1)}, \dots ,\mat{B}^{(d)}]\!].$$ 


where $\mat{B}^{(n)} = \big[\V[]{b}_1^{(n)} \dots \V[]{b}_R^{(n)}\big]$ and $\bm{\lambda} = \big[\lambda_1 \dots \lambda_R\big]^\top$ are called factor matrices and corresponding coefficients.
If we already have a Kruskal tensor input, we can then utilize its special mode-$k$ unfolding representation when solving least squares problem. 
Given a d-way Kruskal tensor $\T{K}$ for example. It's mode-k unfolding in terms of Khatri-Rao product of factor matrices is
$$\mat{X}_{(k)} = \mat{B}_k(\mat{B}_d \odot \dots \mat{B}_{k+1} \odot \mat{B}_{k-1} \odot \dots \odot \mat{B}_1)^\top$$
Which we can reduce computation cost by applying special matrices multiplication.



\section{Algorithm}
\subsection{CP-ALS-NE}
\subsubsection{Algo}


The key to the CP-ALS-NE algorithm is that the problem in each factor matrix is a \emph{linear} least squares problem, which we can solve using linear algebra as described in \cref{sec:LS}.

In terms of the work in CP-ALS-NE, we need to solve a sequence of least square problems of the form 
$$\min_{\mat{\hat{A}}_n}||\mat{X_{(n)}} - {\mat{\hat{A}_n}}\mat{Z}^\top_n ||$$
where $\mat{X_{(n)}}$ is a matricized tensor, $\mat{\hat{A}_n}$ is the variable factor matrix, and $\mat{Z}^\top_n$ is the transpose of the Khatri-Rao product of all the fixed factor matrices:
$$\mat{Z}^\top_n = (\mat{A}_N \odot \mat{A}_{N-1} \odot \dots \odot \mat{A}_{n+1} \odot \mat{A}_{n-1} \odot \dots \odot \mat{A}_1)^\top. $$ 
\GB{We shouldn't use CP-ALS to refer to the normal equations approach, as our QR approach is also based on CP-ALS.  Let's use CP-ALS-NE to refer to the standard approach.  Update this in the text and in \cref{alg:cp-als-ne}.}
In the standard implementations of CP-ALS, this least squares problem is solved using the normal equations approach,
$$\mat{\hat{A}_n}\mat{Z}_n^\top\mat{Z}_n = \mat{X}_{(n)}\mat{Z}_n$$
where $\mat{Z}_n^\top\mat{Z}_n$ can be computed as a Hadamard product of Gram matrices of $\mat{A}_{1}, \dots, \mat{A}_{n-1}, \mat{A}_{n+1}, \dots, \mat{A}_{n}$. 
We refer to this standard approach as CP-ALS-NE. Pseudocode for CP-ALS-NE is given in \cref{alg:cp-als-ne}.
\begin{algorithm}[!ht]
  \caption{CP-ALS-NE}
  \label{alg:cp-als-ne}
  \input{algo/cp_als.tex}
\end{algorithm}
\subsubsection{Dense Input}
\subsubsection{Kruskal Input}
If the input tensor $\T[]{X}$ is already in Kruskal format, we can also use Hadamard product to compute right-hand-side more efficient.
\begin{align}
  \mat{X}_{(n)}\mat{Z}_n &= \mat{X}_n(\mat{X}_N \odot \dots \odot \mat{X}_{n+1} \odot \mat{X}_{n-1} \odot \dots \odot \mat{X}_1)^\top(\mat{A}_N \odot \dots \odot \mat{A}_1) \nonumber \\
 &= \mat{X}_n(\mat{X}_N^\top\mat{A}_N) \ast \dots \ast (\mat{X}_{n+1}^\top\mat{A}_{n+1}) \ast (\mat{X}_{n-1}^\top\mat{A}_{n-1}) \ast \dots \ast (\mat{X}_1^\top\mat{A}_1)  \nonumber 
\end{align}
\subsection{CP-ALS-Explicit-QR}
For CP-ALS-QR approach, it is similar to CP-ALS-NE, but this time more stable QR decomposition is used for solving least square problem instead of normal equation.
For normal N-mode tensor $\T{X}^{I_1 \times \dots \times I_N}$, we still need to solve a bunch of least square problems in the form
$$\min_{\mat{\hat{A}}_n}||\mat{X_{(n)}} - {\mat{\hat{A}_n}}\mat{Z}^\top_n ||$$ for every mode.\\
This time we will first perform QR on each factor matrix in $\mat{Z}_n$, which 
\begin{align}
  \mat{Z}_n &= \mat{A}_N \odot \dots \odot \mat{A}_{n+1} \odot \mat{A}_{n-1} \odot \dots \odot \mat{A}_1 \nonumber \\
  &= (\mat{Q}_N\mat{R}_N) \odot \dots \odot (\mat{Q}_{n+1}\mat{R}_{n+1}) \odot (\mat{Q}_{n-1}\mat{R}_{n-1}) \odot \dots \odot (\mat{Q}_1\mat{R}_1) \nonumber \\
  &= (\mat{Q}_N \otimes \dots \otimes \mat{Q}_{n+1} \otimes \mat{Q}_{n-1} \otimes \dots \otimes \mat{Q}_1)(\mat{R}_N \odot \dots \odot \mat{R}_{n+1} \odot \mat{R}_{n-1} \odot \dots \odot \mat{R}_1)\nonumber  
\end{align}
After that we will perform QR again on Khatri-Rao product of $\mat{R}_{N,\dots,1}$, so $\mat{Z}_n$ will be
$$\mat{Z}_n = (\mat{Q}_N \otimes \dots \otimes \mat{Q}_{n+1} \otimes \mat{Q}_{n-1} \otimes \dots \otimes \mat{Q}_1)\mat{Q}_0\mat{R}_0$$
For solving least square problem, we will first apply kronecker product of Qs $\mat{Q}_N \otimes \dots \otimes \mat{Q}_{n+1} \otimes \mat{Q}_{n-1} \otimes \dots \otimes \mat{Q}_1$ into $\mat{X}_{(n)}$ through TTM  $\mat{Y}_{(n)} =  \T{X} \times_1 \mat{Q}_1^\top \dots \times_{n-1} \mat{Q}_{n-1}^\top \times_{n+1} \mat{Q}_{n+1}^\top \dots \times_N \mat{Q}_N^\top$.
This problem becomes
\begin{align}
  \mat{\hat{A}_n}\mat{Q}_0^\top \mat{R}_0^\top &= \mat{Y}_{(n)} \nonumber \\
  \mat{\hat{A}_n}\mat{R}_0^\top &= \mat{Y}_{(n)} \mat{Q}_0 \nonumber \\
  \mat{\hat{A}_n} &= \mat{R}_0^\top \text{\textbackslash} \mat{Y}_{(n)} \mat{Q}_0 \nonumber
\end{align}
Sample pseudocode for CP-ALS-QR is showed in \cref{alg:cp-als-qr}
\begin{algorithm}
  \caption{CP-ALS-QR}
  \label{alg:cp-als-qr}
  \input{algo/cp_als_qr.tex}
\end{algorithm}
\subsubsection{Dense Input}

\subsubsection{Kruskal Input}



\subsection{CP-ALS-Pairwise-QR}
\subsubsection{Algo}
For Kruskal tensor input, we introduced a new QR-based method utilizing this Kruskal format avoiding forming Multi-TTM tensor $\mat{Y}$ in dense tensor case. 
We can represent Kruskal unfolding into Khatri-Rao product of factor matrices. This will help us to just do simple matrix multiplication.

Take the same least square problem in dense case, we still need to do QR factorization for each factor matrix in $\mat{Z}_n$, which
$$\mat{Z}_n = (\mat{Q}_N \otimes \dots \otimes \mat{Q}_{n+1} \otimes \mat{Q}_{n-1} \otimes \dots \otimes \mat{Q}_1)(\mat{R}_N \odot \dots \odot \mat{R}_{n+1} \odot \mat{R}_{n-1} \odot \dots \odot \mat{R}_1)$$
But this time instead of forming QR for Khatri-Rao product of $\mat{R}_N \odot \dots \odot \mat{R}_{n+1} \odot \mat{R}_{n-1} \odot \dots \odot \mat{R}_1$, we will first do QR for $\mat{R}_N \odot \mat{R}_{N-1} = \mat{Q}_{A}\mat{R}_A$ and Khatri-Rao product of $\mat{R}$ will be 
$$\mat{Q}_{A_{N-1}}\mat{R}_{A_{N-1}} \odot \mat{R}_{N-2} \odot \dots \odot \mat{R}_{n+1} \odot \mat{R}_{n-1} \odot \dots \odot \mat{R}_1$$
We will use the same technique in representing $\mat{Z}_n$, which this will transform into 
$$(\mat{Q}_{A_{N-1}} \otimes \dots \otimes \mat{I}_N \otimes \mat{I}_N \otimes \dots  \otimes  \mat{I}_N)(\mat{R}_{A_{N-1}} \odot \dots \odot \mat{R}_{n+1} \odot \mat{R}_{n-1} \odot \dots \odot \mat{R}_1)$$
By continuing doing this pariwise QR decomposition on $\mat{R}$, we eventually will have a bunch of new QR representations like
$$\mat{V}_N = \underbrace{(\mat{Q}_{A_{N-1}} \otimes \dots  \otimes  \mat{I}_N)}_{N-2}\underbrace{(\mat{Q}_{A_{N-2}} \otimes \dots \otimes \mat{I}_N)}_{N-3} \cdots (\mat{Q}_{A_{2}} \otimes \mat{I}_N) \mat{Q}_{A_{1}}\mat{R}_{A_{1}}$$
where $\mat{Z}_n$ will be 
$$\mat{Z}_n = (\mat{Q}_N \otimes \dots \otimes \mat{Q}_{n+1} \otimes \mat{Q}_{n-1} \otimes \dots \otimes \mat{Q}_1) (\mat{Q}_{A_{N-1}} \otimes \dots  \otimes  \mat{I}_N)(\mat{Q}_{A_{N-2}} \otimes \dots \otimes \mat{I}_N) \cdots (\mat{Q}_{A_{2}} \otimes \mat{I}_N) \mat{Q}_{A_{1}}\mat{R}_{A_{1}}$$

The sample pseudocode is showed in \cref{alg:cp-als-pairwise-qr}
\begin{algorithm}
  \caption{CP-ALS-Pairwise-QR}
  \label{alg:cp-als-pairwise-qr}
  \input{algo/cp_als_pairwise.tex}
\end{algorithm}
\subsubsection{Dense Input}

\subsubsection{Kruskal Input}
We also need to first apply kronecker product of $\mat{Q}$ in $\mat{Z}_n$ to $\mat{X}_n$. 
But this time since we can use the Kruskal structure which $\mat{X}_{(n)} = \mat{B}_{n}(\mat{B}_{N} \odot \dots \odot \mat{B}_{n+1} \odot \mat{B}_{n-1}  \odot \dots \odot \mat{B}_{1})^\top$. 
Applying $\mat{Q}$ into $\mat{X}_n$. 
This gives us
\begin{align}
  \mat{Y}_n &= (\mat{Q}_N \otimes \dots \otimes \mat{Q}_{n+1} \otimes \mat{Q}_{n-1} \otimes \dots \otimes \mat{Q}_1)(\mat{B}_{n}(\mat{B}_{N} \odot \dots \odot \mat{B}_{n+1} \odot \mat{B}_{n-1}  \odot \dots \odot \mat{B}_{1})^\top) \nonumber \\
  &= (\mat{Q}_N\mat{B}_N \odot \dots \odot \mat{Q}_{n+1} \mat{B}_{n+1} \odot \mat{Q}_{n-1}\mat{B}_{n-1} \odot \dots \odot \mat{Q}_{1}\mat{B}_{1})^\top\mat{B}_n \nonumber   
\end{align}
On following step, we will apply that set of kronecker $\mat{Q}$ into $\mat{Y}_n$ in similar pairwise way.
\begin{align}
  \mat{Y}_n   &= (\mat{Q}_N\mat{B}_N \odot \dots \odot \mat{Q}_{n+1} \mat{B}_{n+1} \odot \mat{Q}_{n-1}\mat{B}_{n-1} \odot \dots \odot \mat{Q}_{1}\mat{B}_{1}) \nonumber   \\
      &= ((\mat{Q}_N\mat{B}_N \odot \mat{Q}_{N-1}\mat{B}_{N-1}) \odot \dots \odot \mat{Q}_{n+1} \mat{B}_{n+1} \odot \mat{Q}_{n-1}\mat{B}_{n-1} \odot \dots \odot \mat{Q}_{1}\mat{B}_{1})^\top\mat{B}_n \nonumber      \\
      \mat{Y}_n\mat{Q}_{A_{N-1}}^\top  &= (\mat{Q}_{A_{N-1}}^\top(\mat{Q}_N\mat{B}_N \odot \mat{Q}_{N-1}\mat{B}_{N-1}) \odot \dots \odot \mat{Q}_{n+1} \mat{B}_{n+1} \odot \mat{Q}_{n-1}\mat{B}_{n-1} \odot \dots \odot \mat{Q}_{1}\mat{B}_{1})^\top\mat{B}_n \nonumber \\
      & \vdots \nonumber \\
    \mat{U}_n  &= (\mat{Q}_{M}^\top\mat{B}_M)^\top\mat{B}_n \nonumber  
\end{align}
Notice that we only have to do one matrix multiplication once a time because when use the same property, one matrix times an identity matrix will still be itself.
We can then follow the same way to solve the least square problem with calculated $\mat{Q}^\top\mat{B}$.
\begin{align}
  \mat{\hat{A}}_n \mat{R}^\top_{A_1} &= \mat{B}^\top_M\mat{Q}_M\mat{B}_n \nonumber \\ 
  \mat{\hat{A}}_n &= \mat{R}^\top_{A_1} \text{\textbackslash} \mat{B}^\top_M\mat{Q}_M\mat{B}_n \nonumber 
\end{align}




\subsection{Complexity}

For a d-way Kruskal tensor $\T{K} \in \mathbb{R}^{n_1 \times \dots \times n_d}$ and assume $n_1, \dots , n_d$ are the same. 
If we want to do the rank-$r$ CP Decomposition for two different ALS methods, the distribution of time is presented in \cref{tab:its_parts}


\begin{table}[!ht]
  
  \centering
  \begin{tabular}{|c|c|c|c|c|c|}
    \hline
    \multicolumn{2}{|c|}{\textbf{Normal Equations}} & \multicolumn{2}{|c|}{\textbf{Explicit QR}} & \multicolumn{2}{|c|}{\textbf{Pairwise QR}} \\
    \hline
    Component & Cost & Component & Cost & Component & Cost \\
    \hline
    Computing Gram & $r^2n$&QR on factor matrices & $(d-1)r^2n$ & QR on factor matrices & $(d-1)r^2n$\\
    N/A& &Computing $\mat{Q}_0$ & $r^{d+1}$& Computing $\tilde{\mat{Q}}\tilde{\mat{R}}$& $(d-2)r^4$\\
    Apply RHS&  $r^2n$&Apply Qs to RHS & $(d-1)r^2n$ & Apply Qs to RHS& $(d-1)r^2n$ \\
    N/A & &Applying $\mat{Q}_0$& $r^dn$& Applying $\tilde{\mat{Q}}\tilde{\mat{R}}$& $r^dn$\\
     \hline
  \end{tabular}
  \caption{Breakdown of main component when solving LS problem using normal equation, pairwise QR and explicit QR}
  \label{tab:its_parts}
  \end{table}

  




\section{Result}
To test our new QR-based method, we setup an problem which is sine of sums sin$(x_1+\dots+ x_d)$ approximation problem.
Usually this sine wave can be represented in $2^{d-1}$ term, for example
$$\text{sin}(x_1+x_2+x_3) = \text{sin}(x_1)\text{cos}(x_2)\text{cos}(x_3)+\text{cos}(x_1)\text{cos}(x_2)\text{sin}(x_3)$$
$$+\text{cos}(x_1)\text{sin}(x_2)\text{cos}(x_3) - \text{sin}(x_1)\text{sin}(x_2)\text{sin}(x_3)$$
In this test, we need to approximate this N-dimensional function in a low rank linear representation instead of a exponential one, which by Beylkin and Mohlenkamp,
$$\text{sin}\bigl(\sum^d_{j=1}x_j\bigr) = \sum^d_{j=1}\text{sin}(x_j)\prod^d_{k=1,k\neq j}\frac{\text{sin}(x_k - \alpha_k -\alpha_j)}{\text{sin}(\alpha_k - \alpha_j)}$$
for all choices of  ${\alpha_j}$ such that $\text{sin}(\alpha_k - \alpha_j) \neq 0$ for all $j \neq k$.
\\
We can somehow treat this sine of sum be a N-mode tensor $\T{X} \in \mathbb{R}^{n \times \dots \times n}$, and each term of exponential representation corresponds to a rank $2^{d-1}$ CP decomposition. 
It is also possible to have a rank $n$ CP decomposition by the linear representation mentioned above. 
This representation can be ill-conditioned if $\alpha_k \rightarrow \alpha_j$.
But we also found out that this ill-conditioned property still holds when we choose $\alpha$ to be evenly spaced.

We first used this ill-conditioned problem to solve a least square problem which takes this 6-way exponential sin of sums Kruskal tensor as an input, tries to solve a least square problem on the last factor matrix, and compute the error between original full ktensor and the full ktensor that has been approximated. 
We did visualization for both runtime and accuracy showed in \cref{fig:LS_problem}

\begin{figure}[ht!]
  
  \begin{center}
    \includegraphics*[scale = 0.12]{6stacked.jpg}
    \includegraphics*[scale = 0.22]{6wayacc.jpg}
    \caption[Figure]{runtime and accuracy for pairwise Elim, normal equation, and explicit QR \label{fig:LS_problem}}
  \end{center}
  
  
\end{figure}


Based on these two figures, we can know that new QR-based method has better accuracy than old QR method, and just slightly slower than normal equation because it still has steps for computing and apply QR. 
To be more specific about the relative error, we can observe that in accuracy figure, new QR method is really close to the true error when condition number is $4e11$. 
The relative error through normal equation blows up in this large condition number.
Another thing to point out is that when dimension goes up more than 7, our old QR-based method's error has a sudden increase which I believe it is because of too large Khatri-Rao product formation.


In the next step, I put new QR-based method into CP-ALS which further compare the runtime and accuracy to CP-ALS method using normal equation and Explicit. 
There are also some visualizations showed in \cref{fig:runtime} and \cref{fig:error}
\begin{figure}[ht!]
  \begin{center}
    
    \includegraphics[scale = 0.25]{7runtime.jpg}
    
    \caption[Figure]{runtime for ALS and QR in different dimensions \label{fig:runtime}}
  \end{center}
difference 
\end{figure}
\begin{figure}[ht!]
  \begin{center}
    
    \includegraphics*[scale = 0.3]{accuracy.jpeg}
    \includegraphics*[scale = 0.3]{runtime.jpeg}
    
    \caption[Figure]{runtime and accuracy figure for CP-ALS \label{fig:error}}
  \end{center}
  
\end{figure}

As showed in \cref*{fig:runtime}, for a 7-way sine of sum rank-7 ktensor, the execution time for explicit QR decomposition grows increasing large respect to the rank, but both ALS and pairwise QR technique are more efficient.
For pairwise QR, it is even faster than ALS using normal equation when dimension is 2000. This can be more clearly seen in speedup figure.

In \cref*{fig:error}, the accuracy plot shows that CP-ALS can't achieve relative error much below the square root of machine precision, but CP-ALS-QR gets to around 1e-13. 
The yellow line indicates the difference between input and the actual solution given by the formula. 
It is just a reference which we don't expect both algorithms will get close to it.

The runtime plot indicates that both algorithms have time that depends linearly on n.  
They have different slopes, and in particular the slope of CP-ALS-QR is better, because CP-ALS does redundant computation for each subiteration while our implementation of CP-ALS-QR avoids this. 
So the speedup increases with n (and will also increase with d).
Throughout the test, we used built-in CP-ALS function in tensor toolbox, which there are some redundant computation can be improved.




\section{Conclusion and Future Work}

\section{Appendix}
\subsection{Reflection}
After doing this project in this summer, I really got a sense about how interesting doing research is.
At the beginning of this research, I and professor Ballard first tried another coefficient matrices technique for a few weeks. 
It didn't work well which made me kind of frustrated. 
Luckily, Another professor from England used to think of one different method that after carefully expansion and examination, we found it worked. 
We further tested this method and we were pretty decent about the result.
Then I had to write the report which is less excite because I have to define variables, setup notation and explain how formula and algorithm work. 
My mind in this researching process is just bouncing up and down which is interesting.

For educational development, the main feeling when I was doing research is that I do not have really solid mathematical background. 
Even though I have already took Numerical Linear Algebra and two linear algebra courses in math department, I sometimes still feel like I cannot understand lemmas and properties. 
If I was better at math, I maybe more efficiency and can locate the problem quicker. 
I'm thinking about getting a math minor in the future. 
Besides from this part, I also had a chance to look deep into what I have learnt through this research.
There are lots of mathematical reasons that support these computation methods which I found out really interesting.
For personal development, I think in the future I may get a higher degree because doing research is interesting for now. 
However this doesn't mean that I will always do research.
Right now, I just feel kind of tired because sticking with one thing for 10 more weeks definitely will wipe some excitements from people.
Right now I really want the new semester starts so that I can learn new stuff and take a break from research.
Speaking of this, my future personal goal is try to find a balance between studying and relax, or researching and studying.
\bibliographystyle{plain}


\bibliography{reference}

\end{document}