\documentclass{article}
\title{CP-ALS-QR report}
\author{Alex Zhang}
\date{July 2023}
\textwidth=16.00cm 
\textheight=26.00cm 
\topmargin=0.00cm
\oddsidemargin=0.00cm 
\evensidemargin=0.00cm 
\headheight=0cm 
\headsep=0.5cm
\textheight=610pt
\usepackage{graphicx}
\usepackage{lineno,hyperref}
\usepackage{amsmath,amssymb,enumerate,graphicx,pifont,color,tikz,yfonts,scalerel,changepage,algorithm,algpseudocode}
\usepackage{bookmark}
\usepackage{diagbox}
\usepackage{caption,subcaption}


\usetikzlibrary{positioning}
\usetikzlibrary{cd}

\usepackage{booktabs}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{array}
\usepackage{tikz}
\usepackage{cleveref}


\graphicspath{{fig/}}

\usepackage{latexsym,array,delarray,amsthm,amssymb,epsfig}
\usepackage{amsmath}
\usepackage{listings}
\lstset{
  basicstyle=\ttfamily,
  mathescape
}

\newcommand{\bmat}[1]{\begin{bmatrix} #1 \end{bmatrix}}
\newcommand{\mat}[1]{\mathbf{#1}}
\newcommand{\ten}[1]{\mathcal{#1}}
% matrix/vector/tensor/element macros
\usepackage{bm}
\newcommand{\Tra}{T}										% transpose
\newcommand{\M}[2][]{\bm{#1{\mathbf{\MakeUppercase{#2}}}}} 		% matrix
\newcommand{\Me}[3][]{\bm{#1{\mathbf{\MakeUppercase{#2}}}}({#3})} 		% matrix entry
\newcommand{\Mb}[3][]{\bm{#1{\mathbf{\MakeUppercase{#2}}}}_{#3}}       	% submatrix
\newcommand{\Mbe}[4][]{\bm{#1{\mathbf{\MakeUppercase{#2}}}}_{#3}({#4})}	% submatrix entry
\newcommand{\Ms}[3][]{\bm{#1{\mathbf{\MakeUppercase{#2}}}}^{(#3)}}       	% matrix in series
\newcommand{\Mbs}[4][]{\bm{#1{\mathbf{\MakeUppercase{#2}}}}_{#3}^{(#4)}}   % submatrix in series
\newcommand{\V}[2][]{\bm{#1{\mathbf{\MakeLowercase{#2}}}}} 		% vector
\newcommand{\Vs}[3][]{\bm{#1{\mathbf{\MakeLowercase{#2}}}}^{(#3)}} 		% vector in series
\newcommand{\Ve}[3][]{\bm{#1{\mathbf{\MakeLowercase{#2}}}}({#3})}		% vector entry
\newcommand{\T}[2][]{#1{\mathbf{\cal{#2}}}} 						% tensor
\newcommand{\Te}[3][]{#1{\mathbf{\cal{#2}}}({#3})}	
\DeclareMathOperator*{\hada}{\scalebox{1.5}{$\ast$}}
\let\ds\displaystyle

\newcommand{\GB}[1]{\textcolor{red}{\textbf{GB}: #1}}
\newcommand{\AZ}[1]{\textcolor{blue}{\textbf{AZ}: #1}}
\algnewcommand{\IfThenElse}[3]{\State \algorithmicif\ #1\ \algorithmicthen\ #2\ \algorithmicelse\ #3} % single line: \IfThenElse{<if>}{<then>}{<else>}
\algnewcommand{\IfThen}[2]{\State \algorithmicif\ #1\ \algorithmicthen\ #2} % single line: \IfThen{<if>}{<then>}

\begin{document}



\maketitle
\section{Introduction}
%The CANDECOMP/PARAFAC or canonical polyadic (CP) decomposition for multidimensional data, 
%or tensors, is a popular tool for analyzing and interpreting latent patterns that may be 
%present in multidimensional data. Basically CP decomposition of a tensor refers to its
%expression as a sum of $r$ rank-one components and each of them is a vector outer product.
%One of the most popular methods used to compute a CP decomposition is the alternating least
%squares (CP-ALS) approach, which solves a series of linear least squares problems. Usually
%to solve these linear leaste squares problems, normal equations are used for CP-ALS. This
%approach may be sensitive for ill-conditioned inputs. Based on this idea, there are already
%a more stable approach which is solving the linear least sqaures problems using QR decomposition
%instead.

%For my summer research project, I basically follows the QR apprach but trying to 
%improve the efficiency for QR decomposition when assuming the input tensor is in Kruskal structure,
%that is, a tensor stored as factor matrices and corresponding weights. By exploiting this structure, 
%we improve the computation efficiency by not forming Multi-TTM tensor.
%The problem left is when doing CP-ALS, QR-based methods is exponential in $N$, the number of modes.
%The normal equations approach is linear in $N$ for Kruskal tensor. During the summer I tried
%to revise and implement former QR method which archieved better stability than normal equations
%but computation time increases linearly with respect to $N$.

add space\\
add space\\
add space\\
add space\\
add space\\
add space\\
add space\\
add space\\
add space\\
add space\\
add space\\


\section{Background}
\subsection{CP Decomposition}
Given a $d$-way tensor $\T{X} \in \mathbb{R}^{n_1\times n_2\times \dots \times n_d}$, its
CP decomposition $\T{K}$ of rank $r \in \mathbb{N}$ can be represented as 
\begin{equation}
\label{eq:CP}
\T{X}(i_1,i_2,\dots, i_d) \approx \T{K} = \sum^{r}_{j=1} \V{\lambda}(j) \mat{A_1}(i_1,j)\mat{A_2}(i_2,j) \dots \mat{A_d}(i_d,j)
\end{equation}
for all $(i_1,i_2,\dots, i_d) \in [n_1] \otimes [n_2] \otimes [n_3] \otimes \dots \otimes [n_d]$ where $\mat{A_k} \in \mathbb{R}^{n_k \times r}$ is a factor matrix for each $k \in [d]$ and $\bm{\lambda}\in\mathbb{R}^r$ is a vector of weights. 
\Cref{fig:3d-cp-decomp} gives a visualization of a CP Decomposition for a 3-way tensor.

\begin{figure}[ht!]
\centering
\input{tikz/cp.tex}
\caption{CP decomposition of rank $R$ for a three-dimensional tensor $\T{X}$ \label{fig:3d-cp-decomp}}
\end{figure}
We can represent $\T[]{K}$ in shorthand CP notation as $\T{K} = [\![\V{\lambda};\mat{A}_1,\mat{A}_2, \dots,\mat{A}_d ]\!]$. 
There exists ambiguity in the weighting of the columns of the factor matrices and $\V{\lambda}$ vector.
For simplicity of presentation, we drop the weight vector $\V{\lambda}$ throughout this report and use the representation
$$\T{K} = [\![\mat{A}_1,\mat{A}_2, \dots,\mat{A}_d ]\!].$$

A tensor unfolding is a reshaping of the tensor elements into a matrix; mode-wise unfoldings map tensor fibers of a particular mode to columns of a matrix.
In the case of CP-format tensors like $\T{K}$, the $k$th mode-wise unfolding can be expressed in terms of the factor matrices:
$$\mat{K}_{(k)} = \mat{A}_k (\mat{A}_N \odot \dots \odot \mat{A}_{k+1} \odot \mat{A}_{k-1} \odot \dots \odot \mat{A}_1)^\top,$$
where $\odot$ denotes the Khatri-Rao product.

\subsection{Linear Least Squares Problem}
\label{sec:LS}

In mathematical form, a generic linear least squares problem is given by 
\begin{equation}
\label{eq:LS}
\min_{\mat{X}}||\mat{B} - \mat{X}\mat{A}^\top||_{F}.
\end{equation}
Note that the coefficient matrix $\mat{A}$ appears on the right of the variable matrix, which is the opposite of the standard form.
We use this form to relate to the corresponding least squares problem that arises in the context of computing a CP decomposition.

Several different techniques can be used to solve this problem.
The two we consider here are via the Normal Equations (NE) and by QR decomposition.
The NE specify a (square) linear system of equations and are derived from setting the gradient of the objective function from \cref{eq:LS} to zero, and they involve the Gram of $\mat{A}$ as well as the product $\mat{B}\mat{A}$:
\begin{equation}
\label{eq:NE}
\mat{X}\mat{A}^\top\mat{A} = \mat{B}\mat{A}.
\end{equation}
The NE can be solved via Cholesky factorization of $\mat{A}^\top\mat{A}$ and triangular solves.
The cost of NE depends on the size of $\mat{A}$ and $\mat{B}$. 
For $\mat{A} \in \mathbb{R}^{m \times n}$ and $\mat{B} \in \mathbb{R}^{p \times m}$, the arithmetic complexity of each step of the NE approach is given in \cref{tab:NE-time}.
\begin{table}[!ht]
  \centering
  \begin{tabular}{|c|c|}
    \hline
    Components & Number of flops\\
    \hline
    Compute Gram & $mn^2$ \\
    Compute $\mat{B}\mat{A}$ & $2mnp$\\
    Cholesky factorization & $1/3n^3$ \\
    Back solves & $2pn^2$ \\
    \hline
  \end{tabular}
  \caption{Breakdown of time for using NE to solve \cref{eq:LS} for $\mat{A} \in \mathbb{R}^{m \times n}$ and $\mat{B} \in \mathbb{R}^{p \times m}$}
  \label{tab:NE-time}
\end{table}

In the QR decomposition approach, we first compute the compact QR decomposition $\mat{A} = \mat{Q}\mat{R}$ (so that $\mat{Q}\in \mathbb{R}^{m \times n}$)and then solve the  triangular system
\begin{align}
\label{eq:QR}  
\mat{X} \mat{R}^\top = \mat{B}\mat{Q}.
\end{align}
The time complexity of each step of this approach is given in \cref{tab:QR-time}.
Note that the cost of the QR decomposition assumes that Householder QR is used in the factorization and that the $\mat{Q}$ matrix is formed explicitly.
If $p$ is smaller than $n$, then it is more efficient to leave $\mat{Q}$ in implicit Householder form, reducing the cost of QR to $2mn^2-2/3n^3$.
In this case, we apply $\mat{Q}$ to $\mat{B}$ using Householder transformations, which increases the cost of computing $\mat{B}\mat{Q}$ to $4mnp$. 
%\begin{align}
%  &arg\min_{\mat{X}}||\mat{B} - \mat{X}\mat{A}^\top||_{F}^2 \\
%  &= arg \min_{\mat{X}}||\mat{B} - \mat{X}(\mat{Q}\mat{R})^\top||_{F}^2 \\
%  &= arg \min_{\mat{X}}||\big[\mat{B} - \mat{X}\mat{R}^\top\mat{Q}^\top\big]\big[\mat{Q} \mat{Q}^\bot\big]||_{F}^2 \\
%  &= arg \min_{\mat{X}}||\big[\mat{B}\mat{Q} - \mat{X}\mat{R}^\top \text{    } \mat{B} \mat{Q}^\bot\big]||_{F}^2 \\
%  &= arg \min_{\mat{X}}||\mat{B}\mat{Q} - \mat{X}\mat{R}^\top||_{F}^2 + ||\mat{B}\mat{Q}^\bot||_{F}^2
%\end{align}

\begin{table}[!ht]
  \centering
  \begin{tabular}{|c|c|}
    \hline
    Components & Number of flops\\
    \hline
    Compute QR & $4mn^2 - 4/3n^3$ \\
    Compute $\mat{B}\mat{Q}$ & $2mnp$\\
    N/A & \\
    Back solves & $pn^2$ \\
    \hline
  \end{tabular}
  \caption{Breakdown of time for using QR to solve \cref{eq:LS} for $\mat{A} \in \mathbb{R}^{m \times n}$ and $\mat{B} \in \mathbb{R}^{p \times m}$}
  \label{tab:QR-time}
\end{table}

If $\mat{B}$ has few columns ($p\ll n$), NE is cheaper because of the smaller coefficient in the leading order cost ($mn^2$ vs $2mn^2$). 
However, when $\mat{B}$ has many columns ($p\gg n$), the largest cost is computing $\mat{B}\mat{A}$ for NE and $\mat{B}\mat{Q}$ for QR factorization, which have leading order cost with the same constant ($2mnp$).

Using QR is more numerically stable than NE. 
Let $\kappa = \sigma_1(\mat{A})/\sigma_n(\mat{A})$ be the standard condition number of $\mat{A}$.
If we use QR to solve a least squares problem with a single right hand side; i.e. $\min\|\mat{A}\V{x} - \V{b} \|$, then from \cite[Eq. (19.2)]{trefethen1997numerical} the relative forward error satisfies
\begin{equation}
  \frac{||\V[\tilde]{x} - \V{x}||}{||\V{x}||} = O \biggl(\biggl(\kappa + \frac{\kappa^2\tan\theta}{\eta}\biggr)\epsilon_{mach}\biggr),
\end{equation}
where $\V[\tilde]{x}$ is the computed solution, $\V{x}$ is the exact solution, $\theta$ is the angle between $\mat{A}\V{x}$ and $\V{b}$, and $\eta = \|\mat{A}\|\|\V{x}\|/\|\mat{A}\V{x}\|$.
However, if we use the NE approach, then by \cite[Eq. (19.3)]{trefethen1997numerical} the forward error guarantee can be much larger: 
\begin{equation}
  \frac{||\V[\tilde]{x} - \V{x}||}{||\V{x}||} = O \biggl(\kappa^2 \epsilon_{\text{mach}}\biggr).
\end{equation}
Because of the numerical sensitivity introduced by using the NE approach, the QR approach is much more accurate when $\kappa$ is large (e.g., on the order of $\epsilon_{\text{mach}}^{-1/2}$ or larger).


\subsection{CP-ALS}

\subsubsection{CP Optimization Problem}

When computing a rank-$r$ CP approximation $\T{K}$ for a N-way tensor $\T{X}$, we seek to minimize $\| \T{X} - \T{K} \|$, where the tensor norm generalizes the vector 2-norm or the matrix Frobenius norm.
Expressed using \cref{eq:CP}, we have  
$$\min_{\mat{A}_1,\mat{A}_2, \dots,\mat{A}_d} \| \T{X} - \T{K} \|^2 = \min_{\mat{A}_1,\mat{A}_2, \dots,\mat{A}_d} \sum_{i_1=1}^{n_1} \cdots \sum_{i_d=1}^{n_d} \left(\T{X}(i_1,i_2,\dots, i_d) - \sum^{r}_{j=1} \mat{A_1}(i_1,j)\mat{A_2}(i_2,j) \dots \mat{A_d}(i_d,j) \right)^2. $$
However, this is a nonlinear, nonconvex optimization problem and no closed form solution exists. 
We must resort to using iterative methods to approximate a solution \cite{kolda2009tensor}.

\subsubsection{CP-ALS}

One of the most effective algorithms for computing a CP decomposition of a tensor is called Alternating Least Squares (CP-ALS).
The algorithm works as a block coordinate descent method, where each block of variables is a factor matrix.
That is, it alternates over the factor matrices, holding all but one fixed and computing the optimal solution for the variable matrix.

We can solve each subproblem directly because it is a linear least squares problem.
In updating the $n$th factor matrix, we expose the linear least squares problem by unfolding $\T{X}$ and $\T{K}$ along the $k$th mode:
\begin{align}
\label{eq:cp-als-sub}
\min_{\mat{A}_k}||\T{X} - \T{K}|| &= \min_{\mat{A}_k}||\mat{X}_{(k)} - \mat{K}_{(k)}||_F \notag \\ 
&=  \min_{\mat{A}_k}||\mat{X}_{(k)} - \mat{A}_k(\mat{A}_N \odot \dots \odot \mat{A}_{k+1} \odot \mat{A}_{k-1} \odot \dots \odot \mat{A}_1)^\top||_F \notag \\ 
&=  \min_{\mat{A}_k}\|\mat{X}_{(k)}-\mat{A}_k\mat{Z}_k^\top\|_F,
\end{align}
where $\mat{Z}_k=\mat{A}_N \odot \dots \odot \mat{A}_{k+1} \odot \mat{A}_{k-1} \odot \dots \odot \mat{A}_1$.
Note that the coefficient matrix $\mat{Z}_k^\top$ appears on the right of the variable matrix, matching the generic formulation in \cref{eq:LS}.

All of the algorithms we consider in this report are based on CP-ALS, but they differ in how they solve the linear least squares subproblems specified in \cref{eq:cp-als-sub}.

\subsubsection{CP-Rounding}
\label{sec:cp-rounding}

For some applications, we wish to compute a CP decomposition of a matrix that is already in CP format.
That is, we wish to compute a lower-rank representation of the input tensor, sometimes referred to a CP-Rounding.
If $\T{X} \in \mathbb{R}^{n_1 \times \dots \times n_d}$ is a CP-format tensor with rank $s$, it can be written as 
$$\T{X}(i_1,i_2,\dots, i_d) = \sum^{s}_{j=1} \mat{B}_1(i_1,j)\mat{B}_2(i_2,j) \dots \mat{B}_d(i_d,j)$$
with shorthand notation $\T{X} = [\![\mat{B}^{(1)}, \dots ,\mat{B}^{(d)}]\!]$.

In the case the input tensor is in CP format, we can then utilize its special mode-$k$ unfolding representation 
$$\mat{X}_{(k)} = \mat{B}_k(\mat{B}_d \odot \dots \mat{B}_{k+1} \odot \mat{B}_{k-1} \odot \dots \odot \mat{B}_1)^\top$$
when solving the linear least squares problem given in \cref{eq:cp-als-sub}. 
That is, the least squares subproblem is given by 
\begin{equation}
\label{eq:cp-als-sub-ktensor}
\min_{\mat{A}_k}\|\mat{B}_{k}\mat{Y}_k^\top-\mat{A}_k\mat{Z}_k^\top\|_F,
\end{equation}
where $\mat{Y}_k = \mat{B}_N \odot \dots \odot \mat{B}_{k+1} \odot \mat{B}_{k-1} \odot \dots \odot \mat{B}_1$.
This structure can be exploited in different ways depending on how the least squares problem is solved.


\section{Algorithm}

\GB{need to specify notation/dimensions/etc for cost analyses; let's use $n_1 \times \cdots \times n_d$ as tensor dimension, $\bar n$ is the average dimension so that $d\bar n = \sum_k n_k$ and $\hat n$ is the geometric mean so that $\hat n^d = \prod_k n_k$}
The following explanation and tensor notation are assumed having a tensor in$\in \mathbb{R}^{n_1 \times \cdots \times n_d}$.
We define average dimension for that tensor to be $\bar n$ and its geometric mean to be $\hat n$.
$r$ in this section refers to the rank of CP decomposition, and $s$ means the rank for input CP tensor.
\subsection{CP-ALS-NE}


The key to the CP-ALS-NE algorithm is that we will use the normal equations (NE) method to solve the linear least squares problem given by \cref{eq:cp-als-sub} in each mode.
The normal equations for this problem are given by
\begin{equation}
  \label{eq:CP-NE}
  \mat{\hat{A}_k}\mat{Z}_k^\top\mat{Z}_k = \mat{X}_{(k)}\mat{Z}_k.
\end{equation}
The coefficient matrix $\mat{Z}_k^\top\mat{Z}_k$ can be computed as a Hadamard product of Gram matrices of the Khatri-Rao factors:
\begin{equation*}
\mat{Z}_k^\top\mat{Z}_k %= (\mat{A}_N \odot \dots \odot \mat{A}_{k+1} \odot \mat{A}_{k-1} \odot \dots \odot \mat{A}_1)^\top(\mat{A}_N \odot \dots \odot \mat{A}_{k+1} \odot \mat{A}_{k-1} \odot \dots \odot \mat{A}_1) 
= \mat{A}_d^\top \mat{A}_d \hada \cdots \hada \mat{A}_{k+1}^\top\mat{A}_{k+1} \hada \mat{A}_{k-1}^\top\mat{A}_{k-1} \hada \cdots \hada \mat{A}_{1}^\top\mat{A}_{1}.
\end{equation*}
The right hand side matrix $\mat{X}_{(k)}\mat{Z}_k$ can be computed as a matricized tensor times Khatri-Rao product (MTTKRP).
We refer to this standard approach as CP-ALS-NE. Pseudocode for CP-ALS-NE is given in \cref{alg:cp-als-ne}.
\begin{algorithm}[!ht]
  \caption{CP-ALS-NE}
  \label{alg:cp-als-ne}
  \input{algo/cp_als_ne.tex}
\end{algorithm}

\subsubsection{Dense Input}

When we have a dense tensor input, the dominant cost of each subiteration comes from the MTTKRP computation of $\mat{M}_k = \mat{X}_{(k)}\mat{Z}_k$ in \cref{l:NE-mttkrp}, which has leading order arithmetic cost of $2\hat n^dr$.
Computing $\mat{H}_k$ in \cref{l:NE-Hada} requires $O(dr^2)$ operations, solving the normal equations in \cref{l:NE-solve} costs $O(r^3+n_kr^2)$, and computing the Gram matrix in \cref{l:NE-Gram} costs $O(n_kr^2)$.
Thus, the sum of an entire CP-ALS-NE iteration (updating all factor matrices once) is given by
\begin{equation*}
2d\hat n^dr + O(d\bar n r^2 + d^2r^2 + dr^3).
\end{equation*}
We note that the leading order term can be reduced to $4\hat n^d r$ and the $O(d^2r^2)$ term can be reduced to $O(dr^2)$ using memoization \cite{phan2013fast}. 
\subsubsection{Kruskal Input}
If the input tensor $\T{X}=[\![\mat{B}^{(1)}, \dots ,\mat{B}^{(d)}]\!]$ is already in Kruskal format (see \cref{sec:cp-rounding}), we can compute the MTTKRP more efficiently using the following identity:
\begin{align*}
  \mat{X}_{(k)}\mat{Z}_k &= \mat{B}_k(\mat{B}_d \odot \dots \odot \mat{B}_{k+1} \odot \mat{B}_{k-1} \odot \dots \odot \mat{B}_1)^\top(\mat{A}_d \odot \cdots \mat{A}_{k+1} \odot \mat{A}_{k-1} \odot \cdots \odot \mat{A}_1) \\
 &= \mat{B}_k(\mat{B}_d^\top\mat{A}_d) \hada \cdots \hada (\mat{B}_{k+1}^\top\mat{A}_{k+1}) \hada (\mat{B}_{k-1}^\top\mat{A}_{k-1}) \hada \cdots \hada (\mat{B}_1^\top\mat{A}_1).  
\end{align*}

With this structure, the algorithm for computing CP decomposition with Kruskal tensor will be  
\begin{algorithm}[!ht]
  \caption{CP-Round-ALS-NE}
  \label{alg:cp-als-ne-k}
  \input{algo/cp_als_ne_kruskal.tex}
  
\end{algorithm}

%The cost when having a Kruskal input is basically the same as dense case. Except for $\mat{X}_{(n)}\mat{Z}_n$ (in \cref{l:round-ne-cross} and \cref{l:round-ne-rhs}) which we only need to do hadamard product requires $O(sn_kr)$ instead of $2\hat{n}^dr$.

When the input is a Kruskal tensor of rank $s>r$, the cost is dominated by the computation of the right hand side of the normal equations.
These costs, incurred in \cref{l:round-ne-rhs,l:NE-K-cross}, are $2n_krs+(d-2)rs$ and $n_krs$ in mode $k$, respectively.
Computing the coefficient matrix of the normal equations occurs in \cref{l:round-ne-gram,l:NE-K-Gram} and has costs $(d-2)r^2$ and $n_kr^2$ in mode $k$.
Solving the normal equations in \cref{l:NE-K-solve} costs $2n_kr^2 + O(r^3)$.
Thus, the sum of an entire CP-Round-ALS-NE iteration (updating all factor matrices once) is given by
\begin{equation*}
4d\bar{n}rs + 3d\bar{n}r^2 + O(d^2rs + d^2r^2 + r^3).
\end{equation*}
We note that the $O(d^2rs+d^2r^2)$ terms can be reduced to $O(drs+dr^2)$ using memoization. 


\subsection{CP-ALS-Explicit-QR}
For CP-ALS-QR approach, it is similar to CP-ALS-NE, but this time more stable QR decomposition is used for solving least square problem instead of NE.
For mode-n least squares problem,
$$\min_{\mat{\hat{A}}_k}||\mat{X}_{(k)} - {\mat{\hat{A}}_k}\mat{Z}^\top_k ||$$
We will first perform QR on each factor matrix in $\mat{Z}_k$, which 
\begin{align}
  \mat{Z}_k &= \mat{A}_d \odot \dots \odot \mat{A}_{k+1} \odot \mat{A}_{k-1} \odot \dots \odot \mat{A}_1 \nonumber \\
  &= (\mat{Q}_d\mat{R}_d) \odot \dots \odot (\mat{Q}_{k+1}\mat{R}_{k+1}) \odot (\mat{Q}_{k-1}\mat{R}_{k-1}) \odot \dots \odot (\mat{Q}_1\mat{R}_1) \nonumber \\
  &= (\mat{Q}_d \otimes \dots \otimes \mat{Q}_{k+1} \otimes \mat{Q}_{k-1} \otimes \dots \otimes \mat{Q}_1)(\mat{R}_d \odot \dots \odot \mat{R}_{k+1} \odot \mat{R}_{k-1} \odot \dots \odot \mat{R}_1)\nonumber  
\end{align}
After that we will perform QR again on $\mat{Q}_0\mat{R}_0= (\mat{R}_d \odot \dots \odot \mat{R}_{k+1} \odot \mat{R}_{k-1} \odot \dots \odot \mat{R}_1)$, so $\mat{Z}_k$ will be
$$\mat{Z}_k = (\mat{Q}_d \otimes \dots \otimes \mat{Q}_{k+1} \otimes \mat{Q}_{k-1} \otimes \dots \otimes \mat{Q}_1)\mat{Q}_0\mat{R}_0$$
The triangular system for QR in this case will be 
\begin{equation}
  \hat{\mat{A}}_k\mat{R}_0^\top = \mat{X}_{(k)}(\mat{Q}_d \otimes \dots \otimes \mat{Q}_{k+1} \otimes \mat{Q}_{k-1} \otimes \dots \otimes \mat{Q}_1)\mat{Q}_0 
  \label{eq:CP-EXP-QR}
\end{equation}
For solving least square problem, we will first apply kronecker product of Qs $\mat{Q}_d \otimes \dots \otimes \mat{Q}_{k+1} \otimes \mat{Q}_{k-1} \otimes \dots \otimes \mat{Q}_1$ into $\mat{X}_{(k)}$
$$\mat{Y}_{(k)} = \mat{X}_{(k)}(\mat{Q}_d \otimes \dots \otimes \mat{Q}_{k+1} \otimes \mat{Q}_{k-1} \otimes \dots \otimes \mat{Q}_1)$$
Then use backslash to solve the linear system
\begin{equation}
  \hat{\mat{A}}_k = \mat{R}_0^\top \text{\textbackslash} \mat{Y}_{(k)}\mat{Q}_0
\end{equation}
Sample pseudocode for CP-ALS-QR is showed in \cref{alg:cp-als-qr}
\begin{algorithm}
  \caption{CP-ALS-QR-Exp}
  \label{alg:cp-als-qr}
  \input{algo/cp_als_qr.tex}
\end{algorithm}

\subsubsection{Dense Input}
%The difference between dense and Kruskal tensor is the computation of $\mat{Y}_{(k)}$. 
For dense input, the computation of each subiteration is dominated by \cref{l:EXP-TTM}, which is a multiple tensor-times-matrix (Multi-TTM) computation to form $\T{Y}$ with cost
$2\hat{n}^dr+O(\hat{n}^{d-1}r^2 + \cdots + \hat{n}r^d)$, where we assume the Multi-TTM is performed in order of largest mode to smallest (the largest mode is at least the geometric mean).
The cost of the QR of the Khatri-Rao product of triangular factors in \cref{l:EXP-QR-R} is $4r^{d+1}+O(r^3)$, where we assume $\mat{Q}_0$ is formed explicitly.
The cost of multiplying $\mat{Q}_0$ by $\mat{Y}_{(k)}$ in \cref{l:apply} is $2n_kr^d$.
Solving the triangular system in \cref{l:cp-als-q-exp:solve} costs $n_kr^2$, and computing the QR of the factor matrix in \cref{l:cp-als-q-exp:updateQR} is $4n_kr^2+O(r^3)$, where again we form the orthogonal factor explicitly.
%
%$$\T[]{Y} = \T[]{X} \times_1 \mat{Q}_1^\top \times \dots \times_{k-1} \mat{Q}_{k-1}^\top \times_{k+1}\mat{Q}_{k+1}^\top \times \dots \times_d \mat{Q}_d^\top$$
%and use matricized tensor $\mat{Y}_{(k)}$ times $\mat{Q}_0$.
%The computation of QR for factor matrices in \cref{l:qr_factor_q} takes $(d-2)r^2\bar{n}$. 
%The QR for Khatri-Rao product of $\mat{R}$ in \cref{l:EXP-QR-R} takes $O(r^{d+1})$ overall. 
%The cost for doing Multi-TTM will be $\hat{n}^dr$ and $\mat{U}_k = \mat{Y}_{(k)}\mat{Q}_0$ in \cref{l:apply} will be $\bar{n}r^d$.
Overall, the cost of updating factor matrices along all modes for CP-ALS-QR-Exp is
$$2d\hat{n}^dr + O(d\hat{n}^{d-1}r^2 + \cdots + d\hat{n}r^d + d\bar{n}r^d+ dr^{d+1} + d\bar{n}r^2 + dr^3 ).$$
Note that the leading order term (and several others) can be reduced by a factor of $O(d)$ using memoization \cite{KR19}.

\subsubsection{Kruskal Input}
If we have a Kruskal tensor input, the computation of right hand side will be more efficient by applying cross product for $\mat{Q}_k^\top\mat{B}_k$
\begin{equation}
  \mat{Y}_{(k)}\mat{Q}_0= \mat{B}_k(\mat{Q}_d^\top\mat{B}_d \odot \cdots \odot \mat{Q}_{k+1}^\top\mat{B}_{k+1} \odot \mat{Q}_{k-1}^\top\mat{B}_{k-1} \odot \cdots \odot \mat{Q}_1^\top\mat{B}_1)^\top\mat{Q}_0 \nonumber
\end{equation}
We can treat $\mat{Q}_0$ as a matricized tensor and we can compute this as MTTKRP and a small matrix multiplication.

\begin{algorithm}[!ht]
  \caption{CP-Round-ALS-QR-Exp}
  \label{alg:cp-als-qr-k}
  \input{algo/cp_als_qr_kruskal.tex}
\end{algorithm}

%The cost for this algorithm is also similar to the code in dense case. However, in \cref{l:EXP-K-Apply}, the cost is $O(snr)$.
%and in \cref{l:EXP-mttkrp} the MTTKRP dominates the cost which is $(\hat{n}^dr)$.
In the case of Kruskal input, the costs of \cref{l:cp-round-als-exp-qr,l:cp-round-qr-solve,l:cp-round-qr-updateqr} are identical to the corresponding lines in \cref{alg:cp-als-qr}, with combined cost of $4r^{d+1}+9n_kr^2+O(r^3)$ in mode $k$.
The cost of \cref{l:EXP-mttkrp}, which we implement as an MTTKRP, is $2r^ds$.
The cost of the matrix multiplications in \cref{l:exp-finish-rhs,l:EXP-K-update-cross} together is $4n_krs$.
Thus, the overall cost of CP-Round-ALS-QR-Exp is
$$ 2dr^ds + 4dr^{d+1} + O(d\bar{n}rs+d\bar{n}r^2 + r^3).$$
Again, it is possible to reduce the leading order costs by a factor of $O(d)$ via memoization.

\subsection{CP-ALS-Pairwise-QR}
\subsubsection{Algo}
There is also a new QR-based method instead of computing QR on $(\mat{R}_d \odot \dots \odot \mat{R}_1)$, it compute QR for Khatri-Rao product of two matrices $\mat{R_n} \odot \mat{R}_{n-1}$.
It avoids the formation of $\mat{Q}_0\mat{R}_0$ which its cost is exponential respect to the size of input tensor.
Doing QR on factor matrices is the same.
$$\mat{Z}_k = (\mat{Q}_d \otimes \dots \otimes \mat{Q}_{k+1} \otimes \mat{Q}_{k-1} \otimes \dots \otimes \mat{Q}_1)(\mat{R}_d \odot \dots \odot \mat{R}_{k+1} \odot \mat{R}_{k-1} \odot \dots \odot \mat{R}_1)$$
But this time we will first do QR for $\mat{R}_d \odot \mat{R}_{d-1} = \mat{Q}_{A}\mat{R}_A$ and Khatri-Rao product of $\mat{R}$ will be 
$$\mat{Q}_{A_{d-1}}\mat{R}_{A_{d-1}} \odot \mat{R}_{d-2} \odot \dots \odot \mat{R}_{k+1} \odot \mat{R}_{k-1} \odot \dots \odot \mat{R}_1$$
We will use the same technique in representing $\mat{Z}_k$, which this will transform into 
$$(\mat{Q}_{A_{d-1}} \otimes \dots \otimes \mat{I}_d \otimes \mat{I}_d \otimes \dots  \otimes  \mat{I}_d)(\mat{R}_{A_{d-1}} \odot \dots \odot \mat{R}_{k+1} \odot \mat{R}_{k-1} \odot \dots \odot \mat{R}_1)$$
By continuing doing this pariwise QR decomposition on $\mat{R}$, we eventually will have a bunch of new QR representations like
$$\mat{V}_d = \underbrace{(\mat{Q}_{A_{d-1}} \otimes \dots  \otimes  \mat{I}_d)}_{d-2}\underbrace{(\mat{Q}_{A_{d-2}} \otimes \dots \otimes \mat{I}_d)}_{d-3} \cdots (\mat{Q}_{A_{2}} \otimes \mat{I}_d) \mat{Q}_{A_{1}}\mat{R}_{A_{1}}$$
where $\mat{Z}_k$ will be 
$$\mat{Z}_k = (\mat{Q}_d \otimes \dots \otimes \mat{Q}_{k+1} \otimes \mat{Q}_{k-1} \otimes \dots \otimes \mat{Q}_1) (\mat{Q}_{A_{d-1}} \otimes \dots  \otimes  \mat{I}_d)(\mat{Q}_{A_{d-2}} \otimes \dots \otimes \mat{I}_d) \cdots (\mat{Q}_{A_{2}} \otimes \mat{I}_d) \mat{Q}_{A_{1}}\mat{R}_{A_{1}}$$
We can 
The sample pseudocode is showed in \cref{alg:cp-als-pairwise-qr} 
\begin{algorithm}
  \caption{CP-ALS-QR-Imp}
  \label{alg:cp-als-pairwise-qr}
  \input{algo/cp_als_pairwise.tex}
\end{algorithm}


\subsubsection{Dense Input}

The cost of the factor matrix QR in \cref{l:cp-als-q-exp:updateQR} is $4n_kr^2+O(r^3)$ in mode $k$.
The cost of the Multi-TTM in \cref{l:pairwise-TTM} is the same as in \cref{alg:cp-als-qr}: $2\hat{n}^dr+O(\hat{n}^{d-1}r^2 + \cdots + \hat{n}r^d)$.
The permutation has no arithmetic cost, though in practice it may take significant time due to the high relative cost of memory movement.
The pairwise QR decompositions together cost $4(d-2)r^4 + O(dr^3)$ for mode $k$, and applying the pairwise orthogonal factors via TTM costs a total of $2n_kr^d+2n_kr^{d-1}+\cdots+2n_kr^3$.
The triangular solve in \cref{l:cp-als-q-exp:solve} costs $n_kr^2$.
Thus, the total cost of a full iteration (updating each factor matrix once) of CP-ALS-QR-Imp is
$$ 2d\hat{n}^d + 2d\bar{n}r^d + 4d^2r^4 + O(d\hat{n}^{d-1}r^2 + d\bar{n}r^{d-1}). $$
The leading order cost can be reduced by a factor of $O(d)$ via memoization \cite{KR19}.

\subsubsection{Kruskal Input}

With Kruskal input, we can use the Kruskal structure $\mat{X}_{(k)} = \mat{B}_{k}(\mat{B}_{d} \odot \dots \odot \mat{B}_{k+1} \odot \mat{B}_{k-1}  \odot \dots \odot \mat{B}_{1})^\top$. 
Applying $\mat{Q}$ into $\mat{X}_k$. 
This gives us
\begin{align}
  \mat{Y}_k &= (\mat{B}_{k}(\mat{B}_{d} \odot \dots \odot \mat{B}_{k+1} \odot \mat{B}_{k-1}  \odot \dots \odot \mat{B}_{1})^\top)(\mat{Q}_d \otimes \dots \otimes \mat{Q}_{k+1} \otimes \mat{Q}_{k-1} \otimes \dots \otimes \mat{Q}_1) \nonumber \\
  &= \mat{B}_k(\mat{Q}_d^\top\mat{B}_d \odot \cdots \odot \mat{Q}_{k+1}^\top\mat{B}_{k+1} \odot \mat{Q}_{k-1}^\top\mat{B}_{k-1} \odot \cdots \odot \mat{Q}_1^\top\mat{B}_1)^\top \nonumber
\end{align}
On following step, we will apply that set of kronecker $\mat{Q}$ into $\mat{Y}_k$ in similar pairwise way.
\begin{align}
  \mat{Y}_k &= \mat{B}_k(\mat{Q}_d^\top\mat{B}_d \odot \cdots \odot \mat{Q}_{k+1}^\top\mat{B}_{k+1} \odot \mat{Q}_{k-1}^\top\mat{B}_{k-1} \odot \cdots \odot \mat{Q}_1^\top\mat{B}_1)^\top \nonumber \\
      &= \mat{B}_k((\mat{Q}_d^\top\mat{B}_d \odot \mat{Q}_{d-1}^\top\mat{B}_{d-1}) \odot \dots \odot \mat{Q}_{k+1}^\top\mat{B}_{k+1}\odot \mat{Q}_{k-1}^\top\mat{B}_{k-1} \odot \dots \odot \mat{Q}_{1}^\top\mat{B}_{1})^\top \nonumber      \\
      \mat{Y}_k\mat{Q}_{A_{d-1}}  &= \mat{B}_k(\mat{Q}_{A_{d-1}}((\mat{Q}_d^\top\mat{B}_d \odot \mat{Q}_{d-1}^\top\mat{B}_{d-1}) \odot \dots \odot \mat{Q}_{k+1}^\top\mat{B}_{k+1}\odot \mat{Q}_{k-1}^\top\mat{B}_{k-1} \odot \dots \odot \mat{Q}_{1}^\top\mat{B}_{1}))^\top \nonumber      \\
      & \vdots \nonumber \\
    \mat{U}_k  &= \mat{B}_k(\mat{Q}_M^\top\mat{B}_M)^\top \nonumber  
\end{align}
Notice that we only have to do one matrix multiplication once a time because when use the same property, one matrix times an identity matrix will still be itself.
We can solve this linear system
\begin{equation}
  \mat{\hat{A}}_k\mat{R}_{A_1}^\top = \mat{U}_k 
\end{equation}
The algorithm for CP-ALS-Pairwise-QR in Kruskal input will be
\begin{algorithm}
  \caption{CP-Round-ALS-QR-Imp}
  \label{alg:cp-als-pairwise-qr-kruskal}
  \input{algo/cp_als_pairwise_kruskal.tex}
\end{algorithm}




\GB{New analysis here...}

For Kruskal input of rank $s$, we first consider the costs of the factor matrix QR decompositions and applying the individual orthogonal factors.
In mode $k$, the cost of the QR in \cref{l:Pair-K-update} is $4n_kr^2$, and the cost of applying the explicit orthogonal factor to the corresponding input factor matrix is $2n_krs$.
Performing the pairwise QR decompositions and applying the explicit orthogonal factors occurs in \cref{l:pair-QR-R,l:pair-QR-Rapply}.
For each subiteration, the cost of the two operations, which are both performed $d-2$ times, is $4dr^4+2dr^3s+O(r^4)$.
The final multiplication and triangular solve in \cref{l:Pair-K-matmul,l:Pair-K-solve} together cost $2n_krs+n_kr^2$.
Performing a full iteration and updating each factor matrix once has total cost
$$ 4d\bar{n}rs + 5d\bar{n}r^2 + 2d^2r^3s + 4d^2r^4 + O(r^4). $$
The terms involving factors of $d^2$ can likely be reduced by a factor of $d$ via memoization.

\subsection{Complexity}
Given a dense tensor has the properties at the beginning of this section. 
If we want to do the rank-$r$ CP Decomposition for different ALS methods, the distribution of time is presented in \cref{tab:dense_its_part}


\begin{table}[!ht] 
  \centering
  \begin{tabular}{|c|c|c|c|c|c|}
    \hline
    \multicolumn{2}{|c|}{\textbf{Normal Equations}} & \multicolumn{2}{|c|}{\textbf{Explicit QR}} & \multicolumn{2}{|c|}{\textbf{Implicit QR}} \\
    \hline
    Component & Cost & Component & Cost & Component & Cost \\
    \hline
    Compute RHS &$2d \hat{n}^d r$ &Multi-TTM &$2d\hat n^d r$  & Multi-TTM &$2d\hat n^d r$  \\
    Compute Gram & $d^2r^2 + 3d \bar{n} r^2$&QR on factor matrices & $4d \bar n r^2$ & QR on factor matrices & $4d \bar n r^2$\\
    N/A& &Compute $\mat{Q}_0$ & $4dr^{d+1}$& Compute $\tilde{\mat{Q}}\tilde{\mat{R}}$& $4d^2r^4$\\
    N/A & &Apply $\mat{Q}_0$& $2d\bar n r^d$& Apply $\tilde{\mat{Q}}\tilde{\mat{R}}$& $2d\bar n r^d$\\
    \hline
  \end{tabular}
  \caption{Breakdown of main component when solving LS problem using NE, explicit QR, implicit QR along all modes}
  \label{tab:dense_its_part}
\end{table}






If we already have a Kruskal tensor as input, the distribution of time on different method is showed in \cref{tab:kruskal_its_part}.
\begin{table}[!ht] 
  \centering
  \begin{tabular}{|c|c|c|c|c|c|}
    \hline
    \multicolumn{2}{|c|}{\textbf{Normal Equations}} & \multicolumn{2}{|c|}{\textbf{Explicit QR}} & \multicolumn{2}{|c|}{\textbf{Implicit QR}} \\
    \hline
    Component & Cost & Component & Cost & Component & Cost \\
    \hline
    Compute RHS &$4d\bar n rs$ &  Cross Product&$2d\bar n rs$  & Cross Product &$2d\bar n rs$  \\
    Compute Gram & $d^2r^2+3d\bar n r^2$&QR on factor matrices & $4d \bar n r^2$ & QR on factor matrices & $4d \bar n r^2$\\
    N/A& &Compute $\mat{Q}_0$ & $4dr^{d+1}$& Compute $\tilde{\mat{Q}}\tilde{\mat{R}}$& $4d^2r^4$\\
    N/A & &Apply $\mat{Q}_0$& $2d\bar nrs$& Apply $\tilde{\mat{Q}}\tilde{\mat{R}}$& $2d \bar n rs$\\
    \hline
  \end{tabular}
  \caption{Breakdown of main component given Kruskal tensor input using NE, explicit QR, and implicit QR along all modes}
  \label{tab:kruskal_its_part}
\end{table}


  




\section{Experimental Results}
\subsection*{Explain}

\GB{Okay, these results are a bit of a mess, maybe we need to talk through organization; here are what I think the main ideas we want to illustrate:}
\begin{enumerate}
	\item Explicit QR is slow due to QR decomposition and applying explicit Q
	\begin{itemize}
		\item Fig 3a (7-way case) shows this with a bar breakdown
		\item Table reporting overall times from Fig 2 would also illustrate this (right now figure is kind of useless)
	\end{itemize}
	\item Pairwise QR is comparable to NE in time
	\begin{itemize}
		\item similar to Fig 2, vary $n$ and $d$ combinations with bar plots but leave off Exp QR
	\end{itemize}
	\item Pairwise QR is more accurate than NE
	\begin{itemize}
		\item vary condition number, report backward and forward error, I think you have data for this?
	\end{itemize}
	\item When you plug this into CP-ALS, you get better accuracy and time
	\begin{itemize}
		\item Figure 4 illustrates this for $d=7$
	\end{itemize}
\end{enumerate}
\GB{end of outline}


To test the accuracy and efficiency of the QR-based method, we use a discretization of sine of sums: $\sin(x_1+\dots+ x_d)$.
Repeated use of the textbook sine of sums identity for two variables, we obtain a $2^{d-1}$-term separated representation of the function.
For example, 
\begin{equation*}
\begin{split}
\sin(x_1+x_2+x_3) = \sin(x_1)\cos(x_2)\cos(x_3)+\cos(x_1)\cos(x_2)\sin(x_3) \\
+\cos(x_1)\sin(x_2)\cos(x_3) - \sin(x_1)\sin(x_2)\sin(x_3).
\end{split}
\end{equation*}
\GB{Shouldn't there be two negative terms in this expression?}
As described in previous work \cite{BM02,MVLB23}, there exists a $d$-term separated representation of the form
$$\sin\left(\sum^d_{j=1}x_j\right) = \sum^d_{j=1}\sin(x_j)\prod^d_{k=1,k\neq j}\frac{\sin(x_k - \alpha_k -\alpha_j)}{\sin(\alpha_k - \alpha_j)}$$
for all choices of  ${\alpha_j}$ such that $\sin(\alpha_k - \alpha_j) \neq 0$ for all $j \neq k$.
Our goal is to compute this much more efficient $d$-term representation using the $2^{d-1}$-term representation numerically using discretized representations of the functions.

\subsection*{Single LS problem}

%We can somehow treat this sine of sum be a N-mode tensor $\T{X} \in \mathbb{R}^{n \times \dots \times n}$, and each term of exponential representation corresponds to a rank $2^{d-1}$ CP decomposition. 
%It is also possible to have a rank $n$ CP decomposition by the linear representation mentioned above. 
%This representation can be ill-conditioned if $\alpha_k \rightarrow \alpha_j$.
%In the actual problem setup, we make $a_k$ and $a_j$ to be evenly spaced between $\frac{\pi}{d}\frac{(d-1)}{100}$. 

The goal for this test is to see how the speed and accuracy of different algorithms will change when varying condition number $K$.
We first created bunch of ktensors given different $d$. The least squares problems here are trying to approximate the last factor matrix in exponential representation. Comparing the relative error between approximated matrix and the factor matrix we formed in linear representation through built function. 
The table for showing accuracy is showed in \cref{tab:LS_error}.

\begin{table}[!ht] 
  \centering
  \begin{tabular}{|c|c|c|c|c|c|}
    \hline
    \multicolumn{1}{|c|}{\textbf{d}} & \multicolumn{1}{|c|}{\textbf{n}} & \multicolumn{1}{|c|}{\textbf{K}} & \multicolumn{1}{|c|}{\textbf{NE error}} & \multicolumn{1}{|c|}{\textbf{QR Exp error}} & \multicolumn{1}{|c|}{\textbf{QR Imp error}} \\
    \hline
    3 &1000&$10^4$ &  $1\times 10^{-8}$& $1\times 10^{-12}$& $1\times10^{-12}$ \\
    4 &100 &$10^6$& $7\times 10^{-4}$& $1\times 10^{-10}$& $1\times 10^{-10}$\\
    5&50 &$10^8$& $2\times 10^{-1}$& $2\times 10^{-8}$ & $1\times 10^{-8}$\\
    6 & 25&$10^{10}$& $1\times 10^{-1}$& $2\times 10^{-6}$ & $2\times 10 ^{-6}$\\
    7 & 15 & $10^{12}$ & $4 \times 10^{-1} $&$3\times 10^{-3}$ & $2\times 10^{-4}$\\
    8 & 10& $10^{14}$ & $5\times 10^{-1}$ & $3\times 10^{-2}$ & $2\times 10^{-2}$\\
    \hline
  \end{tabular}
  \caption{Relative error when solving LS problem using NE, QR Exp, and QR Imp}
  \label{tab:LS_error}
\end{table}
We can observe in \cref{tab:LS_error}. As the size of our input tensor increases, the condition number $K$ for LHS matrices increase.
The accuracy using NE starts leveling off when $K = 10^8$ but QR techniques hold the error small.
\AZ{Feels like adding another column showing true error will be nice} 

Figure indicating speed differences is showed in \cref{fig:LS_problem_log}.
\begin{figure}[!ht]
  
  \begin{center}
    \includegraphics*[scale = 0.35]{fig_log_runtime.jpg}
    \caption[Figure]{log runtime respect to QR Imp for NE and Exp in different dimensions \label{fig:LS_problem_log}}

  \end{center}
    
\end{figure}

For speed figure \cref{fig:LS_problem_log}, the time for doing QR Exp still stands out after taking the log.
This is because the time for computing QR on $\mat{R}_N \odot \dots \odot \mat{R}_1$ is exponential to $d$.
The time for doing QR Imp is comparable to NE.
Even in some sizes it is actually faster than NE.
In the next breakdown figure \cref{fig:LS_problem_breakdown}, we will later see where the majority time goes.

\begin{figure}[!ht]
  
  \begin{center}
    \includegraphics*[scale = 0.35]{fig_LS_breakdown.jpg}
    \caption[Figure]{Breakdown of time distributions for NE and QR Imp\label{fig:LS_problem_breakdown}}

  \end{center}
  \AZ{This figure needs to be double checked, feels like the time for Gram/QR is still not right.}
\end{figure}

\subsection*{CP results}





In the next step, we put QR Imp into CP-ALS which further compare the runtime and accuracy to CP-Round-ALS-NE and CP-Round-ALS-QR-Exp. 
This is known that cost of QR Exp is exponential to $d$, which will take longer than the rest when solving a LS problem.
This case is further amplified when doing CP-ALS with more computation needed.
\begin{figure}[ht!]
  \begin{center}
    
    \includegraphics[scale = 0.15]{7runtime.jpg}
    \caption[Figure]{runtime for NE, pairwise QR and Explicit QR in different dimensions \label{fig:runtime}}
    \AZ{Depends on how to count QR}
  \end{center}
\end{figure}


As showed in \cref{fig:runtime}, we created a 7-way sine of sums ktensor and varied its dimentions $n$ to see how each approach cost changes.
On the right, it shows that the execution time for QR Exp always has a large cost despite varying dimensions $n$. 
Both NE and QR Imp approaches are more efficient.
On the left side, we took off QR Exp to closer look at cost for NE and QR Imp. The cost for QR Imp is comparable to NE and it is even faster than NE when dimension is 2000. 
This can be more clearly seen on the right figure of \cref{fig:error}.

In order to further compare the accuracy and time for NE and Pairwise QR algorithm, we put them into a larger dimensions scales from $n = 8$ to $20000$ in \cref{fig:error}.

\begin{figure}[ht!]
  \begin{center}
    
    \includegraphics*[scale = 0.3]{accuracy.jpeg}
    \includegraphics*[scale = 0.3]{runtime.jpeg} 
    
    \caption[Figure]{runtime and accuracy figure for CP-ALS \label{fig:error}}
  \end{center}
  
\end{figure}

In \cref{fig:error}, when $d = 7, n=16$, the accuracy plot shows that CP-ALS can't achieve relative error much below the square root of machine precision, but CP-ALS-QR gets to around 1e-13. 
The yellow line indicates the difference between input and the actual solution given by the formula. 
It is just a reference which we don't expect both algorithms will get close to it.

The runtime plot indicates that both algorithms have time that depends linearly on n.  
They have different slopes, and in particular the slope of CP-ALS-Pairwise-QR is smaller, because CP-ALS-NE does redundant computation for each subiteration while our implementation of pairwise QR avoids this. 
The speedup for pairwise QR increases with n (and will also increase with d).
Throughout the test, we used built-in CP-ALS function in tensor toolbox, which there are some redundant computation can be improved.

\section{Conclusion and Future Work}

\section{Appendix}
\subsection{Reflection}
After doing this project in this summer, I really got a sense about how interesting doing research is.
At the beginning of this research, I and professor Ballard first tried another coefficient matrices technique for a few weeks. 
It didn't work well which made me kind of frustrated. 
Luckily, Another professor from England used to think of one different method that after carefully expansion and examination, we found it worked. 
We further tested this method and we were pretty decent about the result.
Then I had to write the report which is less excite because I have to define variables, setup notation and explain how formula and algorithm work. 
My mind in this researching process is just bouncing up and down which is interesting.

For educational development, the main feeling when I was doing research is that I do not have really solid mathematical background. 
Even though I have already took Numerical Linear Algebra and two linear algebra courses in math department, I sometimes still feel like I cannot understand lemmas and properties. 
If I was better at math, I maybe more efficiency and can locate the problem quicker. 
I'm thinking about getting a math minor in the future. 
Besides from this part, I also had a chance to look deep into what I have learnt through this research.
There are lots of mathematical reasons that support these computation methods which I found out really interesting.
For personal development, I think in the future I may get a higher degree because doing research is interesting for now. 
However this doesn't mean that I will always do research.
Right now, I just feel kind of tired because sticking with one thing for 10 more weeks definitely will wipe some excitements from people.
Right now I really want the new semester starts so that I can learn new stuff and take a break from research.
Speaking of this, my future personal goal is try to find a balance between studying and relax, or researching and studying.
\bibliographystyle{plain}


\bibliography{reference}

\end{document}