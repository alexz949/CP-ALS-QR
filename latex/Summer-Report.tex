\documentclass{article}
\title{CP-ALS-QR report}
\author{Alex Zhang}
\date{July 2023}
\textwidth=16.00cm 
\textheight=26.00cm 
\topmargin=0.00cm
\oddsidemargin=0.00cm 
\evensidemargin=0.00cm 
\headheight=0cm 
\headsep=0.5cm
\textheight=610pt
\usepackage{graphicx}
\usepackage{lineno,hyperref}
\usepackage{amsmath,amssymb,enumerate,graphicx,pifont,color,tikz,yfonts,scalerel,changepage,algorithm,algpseudocode}
\usepackage{bookmark}
\usepackage{diagbox}
\usepackage{caption,subcaption}

\usetikzlibrary{positioning}
\usetikzlibrary{cd}

\usepackage{booktabs}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{array}
\usepackage{tikz}
\usepackage{cleveref}


\graphicspath{{fig/}}

\usepackage{latexsym,array,delarray,amsthm,amssymb,epsfig}
\usepackage{amsmath}
\usepackage{listings}
\lstset{
  basicstyle=\ttfamily,
  mathescape
}

\newcommand{\bmat}[1]{\begin{bmatrix} #1 \end{bmatrix}}
\newcommand{\mat}[1]{\mathbf{#1}}
\newcommand{\ten}[1]{\mathcal{#1}}
% matrix/vector/tensor/element macros
\usepackage{bm}
\newcommand{\Tra}{T}										% transpose
\newcommand{\M}[2][]{\bm{#1{\mathbf{\MakeUppercase{#2}}}}} 		% matrix
\newcommand{\Me}[3][]{\bm{#1{\mathbf{\MakeUppercase{#2}}}}({#3})} 		% matrix entry
\newcommand{\Mb}[3][]{\bm{#1{\mathbf{\MakeUppercase{#2}}}}_{#3}}       	% submatrix
\newcommand{\Mbe}[4][]{\bm{#1{\mathbf{\MakeUppercase{#2}}}}_{#3}({#4})}	% submatrix entry
\newcommand{\Ms}[3][]{\bm{#1{\mathbf{\MakeUppercase{#2}}}}^{(#3)}}       	% matrix in series
\newcommand{\Mbs}[4][]{\bm{#1{\mathbf{\MakeUppercase{#2}}}}_{#3}^{(#4)}}   % submatrix in series
\newcommand{\V}[2][]{\bm{#1{\mathbf{\MakeLowercase{#2}}}}} 		% vector
\newcommand{\Vs}[3][]{\bm{#1{\mathbf{\MakeLowercase{#2}}}}^{(#3)}} 		% vector in series
\newcommand{\Ve}[3][]{\bm{#1{\mathbf{\MakeLowercase{#2}}}}({#3})}		% vector entry
\newcommand{\T}[2][]{#1{\mathbf{\cal{#2}}}} 						% tensor
\newcommand{\Te}[3][]{#1{\mathbf{\cal{#2}}}({#3})}		

\let\ds\displaystyle

\newcommand{\GB}[1]{\textcolor{red}{\textbf{GB}: #1}}


\begin{document}



\maketitle
\section{Introduction}
%The CANDECOMP/PARAFAC or canonical polyadic (CP) decomposition for multidimensional data, 
%or tensors, is a popular tool for analyzing and interpreting latent patterns that may be 
%present in multidimensional data. Basically CP decomposition of a tensor refers to its
%expression as a sum of $r$ rank-one components and each of them is a vector outer product.
%One of the most popular methods used to compute a CP decomposition is the alternating least
%squares (CP-ALS) approach, which solves a series of linear least squares problems. Usually
%to solve these linear leaste squares problems, normal equations are used for CP-ALS. This
%approach may be sensitive for ill-conditioned inputs. Based on this idea, there are already
%a more stable approach which is solving the linear least sqaures problems using QR decomposition
%instead.

%For my summer research project, I basically follows the QR apprach but trying to 
%improve the efficiency for QR decomposition when assuming the input tensor is in Kruskal structure,
%that is, a tensor stored as factor matrices and corresponding weights. By exploiting this structure, 
%we improve the computation efficiency by not forming Multi-TTM tensor.
%The problem left is when doing CP-ALS, QR-based methods is exponential in $N$, the number of modes.
%The normal equations approach is linear in $N$ for Kruskal tensor. During the summer I tried
%to revise and implement former QR method which archieved better stability than normal equations
%but computation time increases linearly with respect to $N$.
copy this senctence till everything is in right position\\
copy this senctence till everything is in right position\\
copy this senctence till everything is in right position\\
copy this senctence till everything is in right position\\
copy this senctence till everything is in right position\\
copy this senctence till everything is in right position\\
copy this senctence till everything is in right position\\
copy this senctence till everything is in right position\\
copy this senctence till everything is in right position\\





\section{Background}
\subsection{CP Decomposition}
Given a $d$-way tensor $\T{X} \in \mathbb{R}^{n_1\times n_2\times \dots \times n_d}$, its
CP decomposition of rank $r \in \mathbb{N}$ can be represented as 
$$\T{X}(i_1,i_2,\dots, i_d) \approx \sum^{r}_{j=1}\mat{A_1}(i_1,j)\mat{A_2}(i_2,j) \dots \mat{A_r}(i_r,j)$$
$$\text{for all } (i_1,i_2,\dots, i_d) \in [n_1] \otimes [n_2] \otimes [n_3] \otimes \dots \otimes [n_d]$$
where $\mat{A_k} \in \mathbb{R}^{n_k \times r}$ is a factor matrix for each $k \in [d]$. 
\Cref{fig:3d-cp-decomp} gives a visualization of a CP Decomposition for a 3-way tensor.

\begin{figure}[ht!]
\centering
\input{tikz/cp.tex}
\caption{CP decomposition of rank $R$ for a three-dimensional tensor $\T{X}$ \label{fig:3d-cp-decomp}}
\end{figure}

\subsection{Linear Least Square Problem}
\label{sec:LS}

In mathematical form, a generic least squares problem is given by 
\begin{equation}
\label{eq:LS}
\min_{\mat{X}}||\mat{B} - \mat{X}\mat{A}^\top||_{F}.
\end{equation}
Note that the coefficient matrix $\mat{A}$ appears on the right of the variable matrix, which is the opposite of the standard form.
We use this form to relate to the corresponding least squares problem that arises in the context of computing a CP decomposition.

Several different techniques can be used to solve this problem.
The two we consider here are via the Normal Equations (NE) and by QR decomposition.
The NE specify a (square) linear system of equations and are derived from setting the gradient of the objective function from \cref{eq:LS} to zero, and they involve the Gram of $\mat{A}$ as well as the product $\mat{B}\mat{A}$:
\begin{equation}
\label{eq:NE}
\mat{X}\mat{A}^\top\mat{A} = \mat{B}\mat{A}
\end{equation}
\GB{I would like you to give the derivation of the NE, but perhaps that's lower priority...}
The NE can be solved via Cholesky factorization of $\mat{A}^\top\mat{A}$ and triangular solves.
\GB{Rewrite the following sentence for the time complexity, it depends on the dimensions of $\mat{B}$, $\mat{A}$, which you didn't specify.}
Which the time complexity for using normal equation will be $O(n^3)$, given $\mat{A} \in \mathbb{R}^{n \times n}$.

\GB{There's lot's wrong here, you can't equate $\mat{X}\mat{A}^\top$ and $\mat{B}$, we are seeking only an approximation.  At the very least, you should claim that solving the linear system $\mat{X}\mat{R}^\top=\mat{B}\mat{Q}$ using triangular solve gives the solution $\mat{X}$. I would prefer you derive it correctly, but again maybe that's lower priority.}
For QR factorization, we will first compute the QR decomposition $\mat{A} = \mat{Q}\mat{R}$ and then apply inverse of $\mat{R}$. In term
of sample problem, we will do
\begin{align}
  \mat{X}\mat{A}^\top &= \mat{B} \nonumber \\
  \mat{X}(\mat{Q}\mat{R})^\top &= \mat{B} \nonumber \\
  \mat{X}\mat{R}^\top\mat{Q}^\top\mat{Q} &= \mat{B}\mat{Q} \nonumber \\
  \mat{X}^\top &= \mat{R}^{-\top}\mat{B}\mat{Q} \nonumber
\end{align} 
\GB{This time complexity needs to be fixed similar to NE, and you should provide constant factors for the leading terms.}
The time complexity for solving least square problem with QR factorization will be $O(n^3)$.


If $\mat{B}$ has few columns, NE is cheaper because of the smaller coefficient in the leading order cost. 
However, when $\mat{B}$ has many columns, the largest cost is computing $\mat{B}\mat{A}$ for NE and $\mat{B}\mat{Q}$ for QR factorization, which are the same.

Using QR is more numerically stable than NE.
\GB{State the bounds on the accuracy of QR and NE explicitly (use your NLA textbook and cite it).  Then you can say in what case it's really important to use QR (i.e., when the condition number is large).}


\subsection{CP-ALS}

\GB{State the approximation problem that we solve in computing a CP (minimizing the residual norm of the tensor approximation).  You can say that this is a nonlinear, nonconvex optimization problem and no closed form solution exists, so we must resort to iterative methods.  Give a citation to the Kolda-Bader survey paper.}

One of the most effective algorithms for computing a CP decomposition of a tensor is called Alternating Least Squares (CP-ALS).
The algorithm works as a block coordinate descent method, where each block of variables is a factor matrix.
That is, it alternates over the factor matrices, holding all but one fixed and computing the optimal solution for the variable matrix.
The key to the CP-ALS algorithm is that the problem in each factor matrix is a \emph{linear} least squares problem, which we can solve using linear algebra as described in \cref{sec:LS}.

In terms of the work in CP-ALS, we need to solve a sequence of least square problems of the form 
$$\min_{\mat{\hat{A}}_n}||\mat{X_{(n)}} - {\mat{\hat{A}_n}}\mat{Z}^\top_n ||$$
where $\mat{X_{(n)}}$ is a matricized tensor, $\mat{\hat{A}_n}$ is the variable factor matrix, and 
$\mat{Z}^\top_n$ is the transpose of the Khatri-Rao product of all the fixed factor matrices:
$$\mat{Z}^\top_n = (\mat{A}_N \odot \mat{A}_{N-1} \odot \dots \odot \mat{A}_{n+1} \odot \mat{A}_{n-1} \odot \dots \odot \mat{A}_1)^\top. $$ 
\GB{We shouldn't use CP-ALS to refer to the normal equations approach, as our QR approach is also based on CP-ALS.  Let's use CP-ALS-NE to refer to the standard approach.  Update this in the text and in \cref{alg:cp-als-ne}.}
In the standard implementations of CP-ALS, this least squares problem is solved using the normal equations approach,
$$\mat{\hat{A}_n}\mat{Z}_n^\top\mat{Z}_n = \mat{X}_{(n)}\mat{Z}_n$$
where $\mat{Z}_n^\top\mat{Z}_n$ can be computed as a Hadamard product of Gram matrices of $\mat{A}_{1}, \dots, \mat{A}_{n-1}, \mat{A}_{n+1}, \dots, \mat{A}_{n}$. 
We refer to this standard approach as CP-ALS-NE.
Pseudocode for CP-ALS-NE is given in \cref{alg:cp-als-ne}.

\begin{algorithm}[!ht]
  \caption{CP-ALS}
  \label{alg:cp-als-ne}
  \input{algo/cp_als.tex}
\end{algorithm}

\subsection{CP-Rounding}

\GB{I started to edit this section, but it's in rough shape.  You don't want to introduce the CP notation here, that should be done earlier in the CP Decomposition section.  You should introduce what notation you'll use to distinguish between input and output.  You should use B's to represent the input to match what you do later in the QR section.  But you need to show how NE works with Kruskal input.  You can leave some details out, but you need the expressions so we can do the complexity analysis for comparison.  You can cite the Kolda-Bader paper on sparse and structured tensors here.  Also, don't switch to 3-way tensors here, you're doing $d$-way everywhere else so stick to that.}

For some applications, we wish to compute a CP decomposition of a matrix that is already in CP or \emph{Kruskal} format.
That is, we wish to compute a lower-rank representation of the input tensor, sometimes referred to a CP-Rounding.
In the case of Kruskal tensor input, the input is represented by a set of factor matrices, which we can write as 
$$\T{K} = [\![\bm{\lambda} ; \mat{A},\mat{B},\mat{C}]\!].$$ 

For a $d$-way Kruskal tensor $\T{X} \in \mathbb{R}^{n_1 \times \dots \times n_d}$, each rank-one 
tensor can be treates as the outer product of $d$ different vectors. Mathematical saying, for a 3-way rank-$R$ Kruskal tensor,
we have the following representation
$$\T{K} = \sum^{R}_{l=1}\V{a}_l \circ\V{b}_l \circ\V{c}_l \in \mathbb{R}^{n_1 \times n_2 \times n_3}$$
where $\V{a}_l$,$\V{b}_l$, and $ \V{c}_l$ are vectors. We can also write this tensor as
$$\T{K} = [\![\bm{\lambda} ; \mat{A},\mat{B},\mat{C}]\!]$$
where $\mat{A},\mat{B},\mat{C}$ are called factor matrices.
\\
Another important usage for Kruskal tensor input is that it has speical mode-$k$ unfolding representation. Given a d-way Kruskal tensor $\T{K}$ for example. It's mode-k unfolding 
in terms of Khatri-Rao product of factor matrices is
$$\mat{K}_{(k)} = \mat{A}_k(\mat{A}_d \odot \dots \mat{A}_{k+1} \odot \mat{A}_{k-1} \odot \dots \odot \mat{A}_1)^\top$$
\subsection{CP-ALS-QR}
For CP-ALS-QR approach, it is similar to CP-ALS, but this time more stable QR decomposition is used for solving least sqaure
problem instead of normal eqaution.
\subsubsection{Dense}
For normal N-mode tensor $\T{X}^{I_1 \times \dots \times I_N}$, we still need to solve a bunch of least sqaure problems in the form
$$\min_{\mat{\hat{A}}_n}||\mat{X_{(n)}} - {\mat{\hat{A}_n}}\mat{Z}^\top_n ||$$
for every mode.\\
This time we will first perform QR on each factor matrix in $\mat{Z}_n$, which 
\begin{align}
  \mat{Z}_n &= \mat{A}_N \odot \dots \odot \mat{A}_{n+1} \odot \mat{A}_{n-1} \odot \dots \odot \mat{A}_1 \nonumber \\
  &= (\mat{Q}_N\mat{R}_N) \odot \dots \odot (\mat{Q}_{n+1}\mat{R}_{n+1}) \odot (\mat{Q}_{n-1}\mat{R}_{n-1}) \odot \dots \odot (\mat{Q}_1\mat{R}_1) \nonumber \\
  &= (\mat{Q}_N \otimes \dots \otimes \mat{Q}_{n+1} \otimes \mat{Q}_{n-1} \otimes \dots \otimes \mat{Q}_1)(\mat{R}_N \odot \dots \odot \mat{R}_{n+1} \odot \mat{R}_{n-1} \odot \dots \odot \mat{R}_1)\nonumber  
\end{align}
After that we will perform QR again on Khatri-Rao prodcut of $\mat{R}_{N,\dots,1}$, so $\mat{Z}_n$ will be
$$\mat{Z}_n = (\mat{Q}_N \otimes \dots \otimes \mat{Q}_{n+1} \otimes \mat{Q}_{n-1} \otimes \dots \otimes \mat{Q}_1)\mat{Q}_0\mat{R}_0$$
For solving least sqaure problem, we will first apply kronecker product of Qs 
$\mat{Q}_N \otimes \dots \otimes \mat{Q}_{n+1} \otimes \mat{Q}_{n-1} \otimes \dots \otimes \mat{Q}_1$ into $\mat{X}_{(n)}$ through TTM 
$\mat{Y}_{(n)} =  \T{X} \times_1 \mat{Q}_1^\top \dots \times_{n-1} \mat{Q}_{n-1}^\top \times_{n+1} \mat{Q}_{n+1}^\top \dots \times_N \mat{Q}_N^\top$.
and this problem becomes
\begin{align}
  \mat{\hat{A}_n}\mat{Q}_0^\top \mat{R}_0^\top &= \mat{Y}_{(n)} \nonumber \\
  \mat{\hat{A}_n}\mat{R}_0^\top &= \mat{Y}_{(n)} \mat{Q}_0 \nonumber \\
  \mat{\hat{A}_n} &= \mat{R}_0^\top \text{\textbackslash} \mat{Y}_{(n)} \mat{Q}_0 \nonumber
\end{align}
Sample pseudocode for CP-ALS-QR is showed in \cref{alg:cp-als-qr}
\begin{algorithm}
  \caption{CP-ALS-QR}
  \label{alg:cp-als-qr}
  \input{algo/cp_als_qr.tex}
\end{algorithm}



\subsubsection{Kruskal}
For Kruskal tensor input, we introduced a new QR-based method utilizing this Kruskal format avoiding
forming Multi-TTM tensor $\mat{Y}$ in dense tensor case. We can represent Kruskal unfolding into
Khatri-Rao product of factor matrices. This will help us to just do simple matrix multiplication.

Take the same least square problem in dense case, we still need to do QR factorization for each factor matrix in $\mat{Z}_n$, which
$$\mat{Z}_n = (\mat{Q}_N \otimes \dots \otimes \mat{Q}_{n+1} \otimes \mat{Q}_{n-1} \otimes \dots \otimes \mat{Q}_1)(\mat{R}_N \odot \dots \odot \mat{R}_{n+1} \odot \mat{R}_{n-1} \odot \dots \odot \mat{R}_1)$$
But this time instead of forming QR for Khatri-Rao product of $\mat{R}_N \odot \dots \odot \mat{R}_{n+1} \odot \mat{R}_{n-1} \odot \dots \odot \mat{R}_1$, we will first do QR for $\mat{R}_N \odot \mat{R}_{N-1} = \mat{Q}_{A}\mat{R}_A$ and Khatri-Rao product of $\mat{R}$ will be 
$$\mat{Q}_{A_{N-1}}\mat{R}_{A_{N-1}} \odot \mat{R}_{N-2} \odot \dots \odot \mat{R}_{n+1} \odot \mat{R}_{n-1} \odot \dots \odot \mat{R}_1$$
We will use the same technique in representing $\mat{Z}_n$, which this will transform into 
$$(\mat{Q}_{A_{N-1}} \otimes \dots \otimes \mat{I}_N \otimes \mat{I}_N \otimes \dots  \otimes  \mat{I}_N)(\mat{R}_{A_{N-1}} \odot \dots \odot \mat{R}_{n+1} \odot \mat{R}_{n-1} \odot \dots \odot \mat{R}_1)$$
By continuing doing this pariwise QR decomposition on $\mat{R}$, we eventually will have a bunch of new QR representations like
$$\mat{V}_N = \underbrace{(\mat{Q}_{A_{N-1}} \otimes \dots  \otimes  \mat{I}_N)}_{N-2}\underbrace{(\mat{Q}_{A_{N-2}} \otimes \dots \otimes \mat{I}_N)}_{N-3} \cdots (\mat{Q}_{A_{2}} \otimes \mat{I}_N) \mat{Q}_{A_{1}}\mat{R}_{A_{1}}$$
where $\mat{Z}_n$ will be 
$$\mat{Z}_n = (\mat{Q}_N \otimes \dots \otimes \mat{Q}_{n+1} \otimes \mat{Q}_{n-1} \otimes \dots \otimes \mat{Q}_1) (\mat{Q}_{A_{N-1}} \otimes \dots  \otimes  \mat{I}_N)(\mat{Q}_{A_{N-2}} \otimes \dots \otimes \mat{I}_N) \cdots (\mat{Q}_{A_{2}} \otimes \mat{I}_N) \mat{Q}_{A_{1}}\mat{R}_{A_{1}}$$
We also need to first apply kronecker product of $\mat{Q}$ in $\mat{Z}_n$ to $\mat{X}_n$. But this time since we 
can use the Kruskal structure which $\mat{X}_{(n)} = \mat{B}_{n}(\mat{B}_{N} \odot \dots \odot \mat{B}_{n+1} \odot \mat{B}_{n-1}  \odot \dots \odot \mat{B}_{1})^\top$. Applying $\mat{Q}$ into 
$\mat{X}_n$. This gives us
\begin{align}
  \mat{Y}_n &= (\mat{Q}_N \otimes \dots \otimes \mat{Q}_{n+1} \otimes \mat{Q}_{n-1} \otimes \dots \otimes \mat{Q}_1)(\mat{B}_{n}(\mat{B}_{N} \odot \dots \odot \mat{B}_{n+1} \odot \mat{B}_{n-1}  \odot \dots \odot \mat{B}_{1})^\top) \nonumber \\
  &= (\mat{Q}_N\mat{B}_N \odot \dots \odot \mat{Q}_{n+1} \mat{B}_{n+1} \odot \mat{Q}_{n-1}\mat{B}_{n-1} \odot \dots \odot \mat{Q}_{1}\mat{B}_{1})^\top\mat{B}_n \nonumber   
\end{align}
On following step, we will apply that set of kronecker $\mat{Q}$ into $\mat{Y}_n$ in similar pairwise way.
\begin{align}
  \mat{Y}_n   &= (\mat{Q}_N\mat{B}_N \odot \dots \odot \mat{Q}_{n+1} \mat{B}_{n+1} \odot \mat{Q}_{n-1}\mat{B}_{n-1} \odot \dots \odot \mat{Q}_{1}\mat{B}_{1}) \nonumber   \\
      &= ((\mat{Q}_N\mat{B}_N \odot \mat{Q}_{N-1}\mat{B}_{N-1}) \odot \dots \odot \mat{Q}_{n+1} \mat{B}_{n+1} \odot \mat{Q}_{n-1}\mat{B}_{n-1} \odot \dots \odot \mat{Q}_{1}\mat{B}_{1})^\top\mat{B}_n \nonumber      \\
      \mat{Y}_n\mat{Q}_{A_{N-1}}^\top  &= (\mat{Q}_{A_{N-1}}^\top(\mat{Q}_N\mat{B}_N \odot \mat{Q}_{N-1}\mat{B}_{N-1}) \odot \dots \odot \mat{Q}_{n+1} \mat{B}_{n+1} \odot \mat{Q}_{n-1}\mat{B}_{n-1} \odot \dots \odot \mat{Q}_{1}\mat{B}_{1})^\top\mat{B}_n \nonumber \\
      & \vdots \nonumber \\
    \mat{U}_n  &= (\mat{Q}_{M}^\top\mat{B}_M)^\top\mat{B}_n \nonumber  
\end{align}
Notice that we only have to do one matrix multiplication once a time because when use the same porperty, one matrix times an identity matrix will still be itself.
We can then follow the same way to solve the least sqaure problem with calculated $\mat{Q}^\top\mat{B}$.
\begin{align}
  \mat{\hat{A}}_n \mat{R}^\top_{A_1} &= \mat{B}^\top_M\mat{Q}_M\mat{B}_n \nonumber \\ 
  \mat{\hat{A}}_n &= \mat{R}^\top_{A_1} \text{\textbackslash} \mat{B}^\top_M\mat{Q}_M\mat{B}_n \nonumber 
\end{align}
The sample pseudocode is showed in \cref{alg:cp-als-pairwise-qr}
\begin{algorithm}
  \caption{CP-ALS-Pairwise-QR}
  \label{alg:cp-als-pairwise-qr}
  \input{algo/cp_als_pairwise.tex}
\end{algorithm}




\subsubsection{Complexity}

For a d-way Kruskal tensor $\T{K} \in \mathbb{R}^{n_1 \times \dots \times n_d}$ and assume $n_1, \dots , n_d$ are the same. If we want to do the rank-$r$ CP Decomposition
for two different ALS methods, the distribution of time is presented in \cref{tab:its_parts}


\begin{table}[!ht]
  
  \centering
  \begin{tabular}{|c|c|c|c|c|c|}
    \hline
    \multicolumn{2}{|c|}{\textbf{Normal Equations}} & \multicolumn{2}{|c|}{\textbf{Explicit QR}} & \multicolumn{2}{|c|}{\textbf{Pairwise QR}} \\
    \hline
    Component & Cost & Component & Cost & Component & Cost \\
    \hline
    Computing Gram & $r^2n$&QR on factor matrices & $(d-1)r^2n$ & QR on factor matrices & $(d-1)r^2n$\\
    N/A& &Computing $\mat{Q}_0$ & $r^{d+1}$& Computing $\tilde{\mat{Q}}\tilde{\mat{R}}$& $(d-2)r^4$\\
    Apply RHS&  $r^2n$&Apply Qs to RHS & $(d-1)r^2n$ & Apply Qs to RHS& $(d-1)r^2n$ \\
    N/A & &Applying $\mat{Q}_0$& $r^dn$& Applying $\tilde{\mat{Q}}\tilde{\mat{R}}$& $r^dn$\\
     \hline
  \end{tabular}
  \caption{Breakdown of main component when solving LS problem using normal equation, pairwise QR and explicit QR}
  \label{tab:its_parts}
  \end{table}

  




\section{Result}
To test our new QR-based method, we setup an problem which is sine of sums sin$(x_1+\dots+ x_d)$ approximation problem.
Usually this sine wave can be represented in $2^{d-1}$ term, for example
$$\text{sin}(x_1+x_2+x_3) = \text{sin}(x_1)\text{cos}(x_2)\text{cos}(x_3)+\text{cos}(x_1)\text{cos}(x_2)\text{sin}(x_3)$$
$$+\text{cos}(x_1)\text{sin}(x_2)\text{cos}(x_3) - \text{sin}(x_1)\text{sin}(x_2)\text{sin}(x_3)$$
In this test, we need to approximate this N-dimensional function in a low rank linear representation instead of 
a exponential one, which by Beylkin and Mohlenkamp,
$$\text{sin}\bigl(\sum^d_{j=1}x_j\bigr) = \sum^d_{j=1}\text{sin}(x_j)\prod^d_{k=1,k\neq j}\frac{\text{sin}(x_k - \alpha_k -\alpha_j)}{\text{sin}(\alpha_k - \alpha_j)}$$
for all choices of  ${\alpha_j}$ such that $\text{sin}(\alpha_k - \alpha_j) \neq 0$ for all $j \neq k$.
\\
We can somehow treat this sine of sum be a N-mode tensor $\T{X} \in \mathbb{R}^{n \times \dots \times n}$, and each term of exponential representation
corresponde to a rank $2^{d-1}$ CP decomposition. It is also possible to have a rank $n$ CP decomposition by the linear
representation mentioned aboved. This representation can be ill-conditioned if $\alpha_k \rightarrow \alpha_j$.
But we also found out that this ill-conditioned porperty still holds when we choose $\alpha$ to be evenly spaced.

We first used this ill-conditioned problem to solve a least sqaure problem which takes this 6-way exponential sin of sums Kruskal tensor as an input,
tries to solve a least square problem on the last factor matrix, and compute the error between original full ktensor 
and the full ktensor that has been approximated. We did visualization for both runtime and accuracy showed in \cref{fig:LS_problem}

\begin{figure}[ht!]
  
  \begin{center}
    \includegraphics*[scale = 0.12]{6stacked.jpg}
    \includegraphics*[scale = 0.22]{6wayacc.jpg}
    \caption[Figure]{runtime and accuracy for pairwise Elim, normal equation, and explicit QR \label{fig:LS_problem}}
  \end{center}
  
  
\end{figure}


Based on these two figures, we can know that new QR-based method has better accuracy than old QR method, and just slightly slower than 
normal equation because it still has steps for computing and apply QR. To be more specific about the relative error, we can observe
that in accuracy figure, new QR method is really close to the true error when condition number is $4e11$. The relative error through normal eqaution
blows up in this large condition number.
Another thing to point out is that when dimension goes up more than 7, our old QR-based method's error has a sudden increase which I believe 
it is because of too large Khatri-Rao prouduct formation.


In the next step, I put new QR-based method into CP-ALS which further compare the runtime and accuracy to 
CP-ALS method using normal equation and Explicit. There are also some visualizations showed in \cref{fig:runtime} and \cref{fig:error}
\begin{figure}[ht!]
  \begin{center}
    
    \includegraphics[scale = 0.25]{7runtime.jpg}
    
    \caption[Figure]{runtime for ALS and QR in different dimensions \label{fig:runtime}}
  \end{center}
difference 
\end{figure}
\begin{figure}[ht!]
  \begin{center}
    
    \includegraphics*[scale = 0.3]{accuracy.jpeg}
    \includegraphics*[scale = 0.3]{runtime.jpeg}
    
    \caption[Figure]{runtime and accuracy figure for CP-ALS \label{fig:error}}
  \end{center}
  
\end{figure}

As showed in \cref*{fig:runtime}, for a 7-way sine of sum rank-7 ktensor, the execution time for explicit QR decomposition
grows increasing large respect to the rank, but both ALS and pairwise QR technique are more efficient.
For pairwise QR, it is even faster than ALS using normal eqaution when dimension is 2000. This can be more clearly seen in speedup figure.

In \cref*{fig:error},
the accuracy plot shows that CP-ALS can't achieve relative error much below the square root 
of machine precision, but CP-ALS-QR gets to around 1e-13. The yellow line indicates the difference
between input and the actual solution given by the formula. It is just a reference which we don't
expect both algorithms will get close to it.

The runtime plot indicates that both algorithms have time that depends linearly on n.  
They have different slopes, and in particular the slope of CP-ALS-QR is better, 
because CP-ALS does redundant computation for each subiteration while our implementation 
of CP-ALS-QR avoids this.  So the speedup increases with n (and will also increase with d).
Throughout the test, we used built-in CP-ALS function in tensor toolbox, which there are some redundant computation 
can be improved.




\section{Conclusion}

\section{Appendix}


\end{document}